{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (15715, 4551)\n",
      "y shape = (15715,)\n",
      "# features = 4551\n",
      "# L words = 15715\n"
     ]
    }
   ],
   "source": [
    "head = 'l30_r15'\n",
    "directory = '../data/'\n",
    "model_fname ='../models/Logistic + L2 (C=1.00) norm l30_r15.pkl'\n",
    "\n",
    "\n",
    "import pickle    \n",
    "from py.utils import load_data\n",
    "\n",
    "x, y, x_words, vocabs = load_data(head, directory)\n",
    "with open(model_fname, 'rb') as f:\n",
    "    classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficient = {vocabs[j]:coef for j, coef in enumerate(classifier.coef_[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('어오고', -0.0035026965225787533),\n",
       " ('어놓는', -0.0017447214860182797),\n",
       " ('느냐고', -0.027019952035748179),\n",
       " ('라느니', 0.0013030948275454116),\n",
       " ('으셔서', -0.3228328145095205)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(coefficient.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from soynlp.utils import get_process_memory\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TrainedNounExtractor:\n",
    "    def __init__(self, coefficient, max_length=8):\n",
    "        self._coef = coefficient\n",
    "        self.lmax = max_length\n",
    "        \n",
    "    def extract(self, sents, min_count=10, min_noun_score=0.1):\n",
    "        self.lrgraph, self.lset, self.rset = self._build_lrgraph(sents, min_count)\n",
    "        self.lentropy, self.rentropy = self._branching_entropy(lset, rset)\n",
    "        scores = self._compute_noun_score(self.lrgraph)\n",
    "        return scores, self.lrgraph, self.lset, self.rset\n",
    "        scores = self._postprocessing(lrgraph, scores)\n",
    "\n",
    "    def _build_lrgraph(self, sents, min_count, pruning_min_count=2):\n",
    "        lset = {}\n",
    "        rset = {}\n",
    "        for n_sent, sent in enumerate(sents):\n",
    "            for eojeol in sent.split():\n",
    "                for e in range(1, min(len(eojeol), self.lmax)+1):\n",
    "                    l = eojeol[:e]\n",
    "                    r = eojeol[e:]\n",
    "                    lset[l] = lset.get(l, 0) + 1\n",
    "                    rset[r] = rset.get(r, 0) + 1\n",
    "            if n_sent % 1000 == 999:\n",
    "                args = (n_sent+1, len(lset), len(rset), get_process_memory())\n",
    "                sys.stdout.write('\\rscaning vocabulary ... %d sents #(l= %d, r= %d), mem= %.3f Gb' % args)\n",
    "            if n_sent % 500000 == 499999:\n",
    "                lset = {l:f for l,f in lset.items() if f >= pruning_min_count}\n",
    "                rset = {l:f for l,f in rset.items() if f >= pruning_min_count}\n",
    "        lset = {l:f for l,f in lset.items() if f >= min_count}\n",
    "        rset = {l:f for l,f in rset.items() if f >= min_count}\n",
    "        \n",
    "        n_sents = n_sent\n",
    "        \n",
    "        lrgraph = {}\n",
    "        for n_sent, sent in enumerate(sents):\n",
    "            for eojeol in sent.split():\n",
    "                for e in range(1, min(len(eojeol), self.lmax)+1):\n",
    "                    l = eojeol[:e]\n",
    "                    r = eojeol[e:]\n",
    "                    if not (l in lset) or not (r in rset):\n",
    "                        continue\n",
    "                    rdict = lrgraph.get(l, {})\n",
    "                    rdict[r] = rdict.get(r, 0) + 1\n",
    "                    lrgraph[l] = rdict            \n",
    "            if n_sent % 1000 == 999:\n",
    "                args = (100*(n_sent+1)/n_sents, '%', n_sent+1, n_sents, get_process_memory())\n",
    "                sys.stdout.write('\\rbuilding lrgraph ... (%.3f %s, %d in %d), mem= %.3f Gb' % args)\n",
    "        args = (len(lset), len(rset), sum((len(rdict) for rdict in lrgraph.values())), get_process_memory())\n",
    "        print('\\rlrgraph has been built. (#L= %d, #R= %d, #E=%d), mem= %.3f Gb' % args)\n",
    "        return lrgraph, lset, rset\n",
    "    \n",
    "    def _branching_entropy(self, lset, rset):\n",
    "        def entropy(d):\n",
    "            sum_ = sum(d.values())\n",
    "            if sum_ == 0: return 0\n",
    "            return -1 * sum((v/sum_) * np.log(v/sum_) for v in d.values())\n",
    "        def to_branching_map(d, get_root=lambda x:x[:-1]):\n",
    "            tree = {}\n",
    "            for w,f in d.items():\n",
    "                if len(w) <= 1: continue\n",
    "                root = get_root(w)\n",
    "                branch = tree.get(root, {})\n",
    "                branch[w] = branch.get(w, 0) + f\n",
    "                tree[root] = branch\n",
    "            return tree\n",
    "        print('compute branching entropy ...', end='')\n",
    "        lentropy = {l:entropy(branch) for l, branch in to_branching_map(lset).items()}\n",
    "        rentropy = {r:entropy(branch) for r, branch in to_branching_map(rset, get_root=lambda x:x[1:]).items()}    \n",
    "        print(' done')\n",
    "        return lentropy, rentropy\n",
    "        \n",
    "    def _compute_noun_score(self, lrgraph):\n",
    "        from collections import namedtuple\n",
    "        Score = namedtuple('Score', 'score frequency branching_entropy feature_fraction eojeol_fraction')\n",
    "        scores = {}\n",
    "        n = len(lrgraph)\n",
    "        for i, (l, rdict) in enumerate(lrgraph.items()):\n",
    "            rdict_ = {r:f for r,f in rdict.items() if r in self._coef}\n",
    "            rsum = sum((f for r,f in rdict.items() if r != ''))\n",
    "            frequency = rsum + rdict.get('', 0)\n",
    "            feature_fraction = sum(rdict_.values()) / rsum if rsum > 0 else 0\n",
    "            eojeol_fraction = 1 - rsum / frequency\n",
    "            if not rdict_:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = sum(f*self._coef[r] for r, f in rdict_.items()) / sum(rdict_.values())\n",
    "            scores[l] = Score(score, frequency, self.lentropy.get(l, 0), feature_fraction, eojeol_fraction)\n",
    "            if (i+1) % 1000 == 0:\n",
    "                args = (100*(i+1)/n, '%', i+1, n)\n",
    "                sys.stdout.write('\\rcompute noun score ... (%.3f %s, %d in %d)' % args)\n",
    "        print('\\rcomputing noun score has been done.')\n",
    "        return sorted(scores.items(), key=lambda x:x[1].score, reverse=True)\n",
    "        \n",
    "    def _postprocessing(self, lrgraph, scores):\n",
    "        print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrgraph has been built. (#L= 201559, #R= 86324, #E=2487566), mem= 2.068 Gb\n",
      "compute branching entropy ... done\n",
      "computing noun score has been done.\n"
     ]
    }
   ],
   "source": [
    "from config import sentence_fname\n",
    "class Sentences:\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield doc.strip()\n",
    "\n",
    "sentences = Sentences(sentence_fname)\n",
    "noun_extractor = TrainedNounExtractor(coefficient)\n",
    "scores, lrgraph, lset, rset = noun_extractor.extract(sentences)\n",
    "\n",
    "lentropy, rentropy = noun_extractor.lentropy, noun_extractor.rentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor, Postprocessing의 필요성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lrgraph를 만들 때 한글 외의 기호를 삭제한 어절로 이뤄져야 함 \n",
    "- N = Nsub + J problem\n",
    "    - 중세보편주 + {의, 의의}\n",
    "- [N+Jsub] + J problem\n",
    "    - 대학생과 + {'', 의}\n",
    "    - {의, 과의} 모두 조사\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'의': 6, '의를': 7, '의의': 2}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgraph['중세보편주']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 24, ',': 1, '의': 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgraph['대학생과']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 165),\n",
       " ('이', 74),\n",
       " ('들이', 73),\n",
       " ('들의', 67),\n",
       " ('들은', 36),\n",
       " ('의', 26),\n",
       " ('을', 26),\n",
       " ('과', 24),\n",
       " ('들을', 23),\n",
       " ('은', 21)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lrgraph['대학생'].items(), key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('개인과',\n",
       "  Score(score=6.2108250932929217, frequency=124, branching_entropy=0, feature_fraction=0.75, eojeol_fraction=0.967741935483871)),\n",
       " ('로버트',\n",
       "  Score(score=6.2108250932929217, frequency=175, branching_entropy=0, feature_fraction=0.25, eojeol_fraction=0.9771428571428571)),\n",
       " ('자유주',\n",
       "  Score(score=6.2108250932929217, frequency=182, branching_entropy=-0.0, feature_fraction=0.14835164835164835, eojeol_fraction=0.0)),\n",
       " ('한낱',\n",
       "  Score(score=6.2108250932929217, frequency=127, branching_entropy=0, feature_fraction=1.0, eojeol_fraction=0.984251968503937)),\n",
       " ('우르',\n",
       "  Score(score=6.2108250932929217, frequency=158, branching_entropy=0.40846145222649699, feature_fraction=0.006369426751592357, eojeol_fraction=0.006329113924050667)),\n",
       " ('그런대로',\n",
       "  Score(score=6.2108250932929217, frequency=188, branching_entropy=0, feature_fraction=0.5, eojeol_fraction=0.9893617021276596)),\n",
       " ('당신과',\n",
       "  Score(score=6.2108250932929217, frequency=128, branching_entropy=0, feature_fraction=0.8888888888888888, eojeol_fraction=0.9296875)),\n",
       " ('낭만주',\n",
       "  Score(score=6.2108250932929217, frequency=179, branching_entropy=-0.0, feature_fraction=0.22346368715083798, eojeol_fraction=0.0)),\n",
       " ('여지껏',\n",
       "  Score(score=6.2108250932929217, frequency=106, branching_entropy=0, feature_fraction=0.3333333333333333, eojeol_fraction=0.9716981132075472)),\n",
       " ('지금껏',\n",
       "  Score(score=6.2108250932929217, frequency=177, branching_entropy=0, feature_fraction=0.5, eojeol_fraction=0.9887005649717514))]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x:x[1].frequency > 100 and len(x[0]) > 1, scores))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('좌익과',\n",
       "  Score(score=6.2108250932929217, frequency=17, branching_entropy=0, feature_fraction=1.0, eojeol_fraction=0.8235294117647058)),\n",
       " ('과거부터',\n",
       "  Score(score=6.2108250932929217, frequency=22, branching_entropy=0, feature_fraction=1.0, eojeol_fraction=0.9545454545454546)),\n",
       " ('<시인',\n",
       "  Score(score=6.2108250932929217, frequency=17, branching_entropy=0, feature_fraction=0.5, eojeol_fraction=0.8823529411764706)),\n",
       " ('負',\n",
       "  Score(score=6.2108250932929217, frequency=3, branching_entropy=0, feature_fraction=1.0, eojeol_fraction=0.0)),\n",
       " ('<밤',\n",
       "  Score(score=6.2108250932929217, frequency=4, branching_entropy=0, feature_fraction=0.25, eojeol_fraction=0.0)),\n",
       " ('각권',\n",
       "  Score(score=6.2108250932929217, frequency=17, branching_entropy=0, feature_fraction=1.0, eojeol_fraction=0.9411764705882353)),\n",
       " ('개인과',\n",
       "  Score(score=6.2108250932929217, frequency=124, branching_entropy=0, feature_fraction=0.75, eojeol_fraction=0.967741935483871)),\n",
       " ('자와',\n",
       "  Score(score=6.2108250932929217, frequency=82, branching_entropy=0, feature_fraction=0.7142857142857143, eojeol_fraction=0.9146341463414634)),\n",
       " ('진료와',\n",
       "  Score(score=6.2108250932929217, frequency=11, branching_entropy=0, feature_fraction=1.0, eojeol_fraction=0.9090909090909091)),\n",
       " ('로버트',\n",
       "  Score(score=6.2108250932929217, frequency=175, branching_entropy=0, feature_fraction=0.25, eojeol_fraction=0.9771428571428571))]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(scores)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev: post-processing\n",
    "\n",
    "### branching entropy는 좀 구해볼까? \n",
    "    \n",
    "    L, right-side \n",
    "    R, left-side\n",
    "    \n",
    "    ## Complete\n",
    "    \n",
    "    >>> for w in ['대학', '대학생', '대학생과']:\n",
    "    >>>     print(w, '%.3f' % lentropy.get(w, 0))\n",
    "\n",
    "    대학 2.832\n",
    "    대학생 1.721\n",
    "    대학생과 0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(score=1, frequency=67, branching_entropy=0, feature_fraction=0.761, eojeol_fraction=0.000)\n",
      "Score(score=2, frequency=64, branching_entropy=0, feature_fraction=0.739, eojeol_fraction=0.281)\n"
     ]
    }
   ],
   "source": [
    "score_dict = dict(scores)\n",
    "\n",
    "# useful function\n",
    "def pretty(namedtuple_instance):\n",
    "    print('%s(%s)' % (namedtuple_instance.__class__.__name__, ', '.join(['%s=%.3f' % (field, value) if type(value) == float else '%s=%d' % (field, value) for field, value in namedtuple_instance._asdict().items()])))\n",
    "    \n",
    "pretty(score_dict['떡볶'])\n",
    "pretty(score_dict['떡볶이'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nsub + J: 떡볶 + 이\n",
    "\n",
    "    f(떡볶) ~= f(떡볶이): drop-rate, branching entropy로 하자\n",
    "    (떡볶이 in Noun) and (이 in Josa)\n",
    "    주로 한글자 짜리 조사가 문제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [N + Jsub] + J: 대학생과 + 의\n",
    "\n",
    "    f(대학생) >> f(대학생과)\n",
    "    Right side branching entropy(대학생) ~= high\n",
    "    (과의 in Josa) and (대학생 in Noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound: 소수 + [집단 + 의]\n",
    "\n",
    "    소수 + 집단의 (집단의 = 집단 + 의 인지 확인)\n",
    "    Noun score 대체\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
