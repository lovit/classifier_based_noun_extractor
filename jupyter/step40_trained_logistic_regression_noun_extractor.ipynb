{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (15715, 4551)\n",
      "y shape = (15715,)\n",
      "# features = 4551\n",
      "# L words = 15715\n"
     ]
    }
   ],
   "source": [
    "head = 'l30_r15'\n",
    "directory = '../data/'\n",
    "model_fname ='../models/Logistic + L2 (C=1.00) norm l30_r15.pkl'\n",
    "\n",
    "\n",
    "import pickle    \n",
    "from py.utils import load_data\n",
    "\n",
    "x, y, x_words, vocabs = load_data(head, directory)\n",
    "with open(model_fname, 'rb') as f:\n",
    "    classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficient = {vocabs[j]:coef for j, coef in enumerate(classifier.coef_[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('어오고', -0.0035026965225787533),\n",
       " ('어놓는', -0.0017447214860182797),\n",
       " ('느냐고', -0.027019952035748179),\n",
       " ('라느니', 0.0013030948275454116),\n",
       " ('으셔서', -0.3228328145095205)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(coefficient.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from soynlp.utils import get_process_memory\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TrainedNounExtractor:\n",
    "    def __init__(self, coefficient, max_length=8):\n",
    "        self._coef = coefficient\n",
    "        self.lmax = max_length\n",
    "        \n",
    "    def extract(self, sents, min_count=10, min_noun_score=0.1):\n",
    "        self.lrgraph, self.lset, self.rset = self._build_lrgraph(sents, min_count)\n",
    "        self.lentropy, self.rentropy = self._branching_entropy(lrgraph)\n",
    "        scores = self._compute_noun_score(self.lrgraph)\n",
    "        return scores, self.lrgraph\n",
    "        scores = self._postprocessing(lrgraph, scores)\n",
    "\n",
    "    def _build_lrgraph(self, sents, min_count, pruning_min_count=2):\n",
    "        lset = {}\n",
    "        rset = {}\n",
    "        for n_sent, sent in enumerate(sents):\n",
    "            for eojeol in sent.split():\n",
    "                for e in range(1, min(len(eojeol), self.lmax)+1):\n",
    "                    l = eojeol[:e]\n",
    "                    r = eojeol[e:]\n",
    "                    lset[l] = lset.get(l, 0) + 1\n",
    "                    rset[r] = rset.get(r, 0) + 1\n",
    "            if n_sent % 1000 == 999:\n",
    "                args = (n_sent+1, len(lset), len(rset), get_process_memory())\n",
    "                sys.stdout.write('\\rscaning vocabulary ... %d sents #(l= %d, r= %d), mem= %.3f Gb' % args)\n",
    "            if n_sent % 500000 == 499999:\n",
    "                lset = {l:f for l,f in lset.items() if f >= pruning_min_count}\n",
    "                rset = {l:f for l,f in rset.items() if f >= pruning_min_count}\n",
    "        lset = {l:f for l,f in lset.items() if f >= min_count}\n",
    "        rset = {l:f for l,f in rset.items() if f >= min_count}\n",
    "        \n",
    "        n_sents = n_sent\n",
    "        \n",
    "        lrgraph = {}\n",
    "        for n_sent, sent in enumerate(sents):\n",
    "            for eojeol in sent.split():\n",
    "                for e in range(1, min(len(eojeol), self.lmax)+1):\n",
    "                    l = eojeol[:e]\n",
    "                    r = eojeol[e:]\n",
    "                    if not (l in lset) or not (r in rset):\n",
    "                        continue\n",
    "                    rdict = lrgraph.get(l, {})\n",
    "                    rdict[r] = rdict.get(r, 0) + 1\n",
    "                    lrgraph[l] = rdict            \n",
    "            if n_sent % 1000 == 999:\n",
    "                args = (100*(n_sent+1)/n_sents, '%', n_sent+1, n_sents, get_process_memory())\n",
    "                sys.stdout.write('\\rbuilding lrgraph ... (%.3f %s, %d in %d), mem= %.3f Gb' % args)\n",
    "        args = (len(lset), len(rset), sum((len(rdict) for rdict in lrgraph.values())), get_process_memory())\n",
    "        print('\\rlrgraph has been built. (#L= %d, #R= %d, #E=%d), mem= %.3f Gb' % args)\n",
    "        return lrgraph, lset, rset\n",
    "    \n",
    "    def _branching_entropy(self, lrgraph):\n",
    "        from collections import defaultdict\n",
    "        def entropy(d):\n",
    "            sum_ = sum(d.values())\n",
    "            if sum_ == 0: return 0\n",
    "            return -1 * sum((v/sum_) * np.log(v/sum_) for v in d.values())\n",
    "        def branch_map(d, get_branch):\n",
    "            b = defaultdict(lambda: 0)\n",
    "            for ext,f in d.items():\n",
    "                if ext != '':\n",
    "                    b[get_branch(ext)] += f\n",
    "            return b\n",
    "        def all_entropy(graph, get_branch=lambda x:x[0]):\n",
    "            return {w:entropy(branch_map(d, get_branch)) for w, d in graph.items()}\n",
    "        def to_rlgraph(lrgraph):\n",
    "            rlgraph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "            for l, rdict in lrgraph.items():\n",
    "                for r, f in rdict.items():\n",
    "                    rlgraph[r][l] += f\n",
    "            return {r:dict(ldict) for r, ldict in rlgraph.items()}    \n",
    "        print('compute branching entropy ...', end='')\n",
    "        lentropy = all_entropy(lrgraph)\n",
    "        rentropy = all_entropy(to_rlgraph(lrgraph), get_branch=lambda x:x[-1])\n",
    "        print(' done')\n",
    "        return lentropy, rentropy\n",
    "        \n",
    "    def _compute_noun_score(self, lrgraph):\n",
    "        from collections import namedtuple\n",
    "        Score = namedtuple('Score', 'score frequency branching_entropy feature_fraction eojeol_fraction')\n",
    "        scores = {}\n",
    "        n = len(lrgraph)\n",
    "        for i, (l, rdict) in enumerate(lrgraph.items()):\n",
    "            rdict_ = {r:f for r,f in rdict.items() if r in self._coef}\n",
    "            rsum = sum((f for r,f in rdict.items() if r != ''))\n",
    "            frequency = rsum + rdict.get('', 0)\n",
    "            feature_fraction = sum(rdict_.values()) / rsum if rsum > 0 else 0\n",
    "            eojeol_fraction = 1 - rsum / frequency\n",
    "            if not rdict_:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = sum(f*self._coef[r] for r, f in rdict_.items()) / sum(rdict_.values())\n",
    "            scores[l] = Score(score, frequency, self.lentropy.get(l, 0), feature_fraction, eojeol_fraction)\n",
    "            if (i+1) % 1000 == 0:\n",
    "                args = (100*(i+1)/n, '%', i+1, n)\n",
    "                sys.stdout.write('\\rcompute noun score ... (%.3f %s, %d in %d)' % args)\n",
    "        print('\\rcomputing noun score has been done.')\n",
    "        return sorted(scores.items(), key=lambda x:x[1].score, reverse=True)\n",
    "        \n",
    "    def _postprocessing(self, lrgraph, scores):\n",
    "        print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrgraph has been built. (#L= 201559, #R= 86324, #E=2487566), mem= 2.063 Gb\n",
      "compute branching entropy ... done\n",
      "computing noun score has been done.\n"
     ]
    }
   ],
   "source": [
    "from config import sentence_fname\n",
    "class Sentences:\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield doc.strip()\n",
    "\n",
    "sentences = Sentences(sentence_fname)\n",
    "noun_extractor = TrainedNounExtractor(coefficient)\n",
    "scores, lrgraph = noun_extractor.extract(sentences)\n",
    "\n",
    "lset, rset = noun_extractor.lset, noun_extractor.rset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_rlgraph(lrgraph):\n",
    "    from collections import defaultdict\n",
    "    rlgraph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for l, rdict in lrgraph.items():\n",
    "        for r, f in rdict.items():\n",
    "            rlgraph[r][l] += f\n",
    "    return {r:dict(ldict) for r, ldict in rlgraph.items()}\n",
    "\n",
    "rlgraph = to_rlgraph(lrgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor, Postprocessing의 필요성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lrgraph를 만들 때 한글 외의 기호를 삭제한 어절로 이뤄져야 함 \n",
    "- N = Nsub + J problem\n",
    "    - 중세보편주 + {의, 의의}\n",
    "- [N+Jsub] + J problem\n",
    "    - 대학생과 + {'', 의}\n",
    "    - {의, 과의} 모두 조사\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful function\n",
    "def pretty(namedtuple_instance, end='\\n'):\n",
    "    print('%s(%s)' % (namedtuple_instance.__class__.__name__, ', '.join(['%s=%d' % (field, value) if (type(value) == int or type(value) == np.int) else '%s=%.3f' % (field, value)  for field, value in namedtuple_instance._asdict().items()])),end=end )\n",
    "\n",
    "def pretty_list(l):\n",
    "    print('[', end='')\n",
    "    for w, nt in l[:10]:\n",
    "        print('(%s,'%w, end=' ')\n",
    "        pretty(nt, end='),\\n')\n",
    "    print(']' if len(l) <= 10 else '... ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'의': 6, '의를': 7, '의의': 2}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgraph['중세보편주']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 24, ',': 1, '의': 1}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgraph['대학생과']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 165),\n",
       " ('이', 74),\n",
       " ('들이', 73),\n",
       " ('들의', 67),\n",
       " ('들은', 36),\n",
       " ('의', 26),\n",
       " ('을', 26),\n",
       " ('과', 24),\n",
       " ('들을', 23),\n",
       " ('은', 21)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lrgraph['대학생'].items(), key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(개인과, Score(score=6.211, frequency=124, branching_entropy=0.562, feature_fraction=0.750, eojeol_fraction=0.968)),\n",
      "(로버트, Score(score=6.211, frequency=175, branching_entropy=0.562, feature_fraction=0.250, eojeol_fraction=0.977)),\n",
      "(자유주, Score(score=6.211, frequency=182, branching_entropy=-0.000, feature_fraction=0.148, eojeol_fraction=0.000)),\n",
      "(한낱, Score(score=6.211, frequency=127, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.984)),\n",
      "(우르, Score(score=6.211, frequency=158, branching_entropy=0.651, feature_fraction=0.006, eojeol_fraction=0.006)),\n",
      "(그런대로, Score(score=6.211, frequency=188, branching_entropy=0.693, feature_fraction=0.500, eojeol_fraction=0.989)),\n",
      "(당신과, Score(score=6.211, frequency=128, branching_entropy=0.349, feature_fraction=0.889, eojeol_fraction=0.930)),\n",
      "(낭만주, Score(score=6.211, frequency=179, branching_entropy=-0.000, feature_fraction=0.223, eojeol_fraction=0.000)),\n",
      "(여지껏, Score(score=6.211, frequency=106, branching_entropy=0.637, feature_fraction=0.333, eojeol_fraction=0.972)),\n",
      "(지금껏, Score(score=6.211, frequency=177, branching_entropy=0.693, feature_fraction=0.500, eojeol_fraction=0.989)),\n",
      "... ]\n"
     ]
    }
   ],
   "source": [
    "pretty_list(list(filter(lambda x:x[1].frequency > 100 and len(x[0]) > 1, scores))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(좌익과, Score(score=6.211, frequency=17, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.824)),\n",
      "(과거부터, Score(score=6.211, frequency=22, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.955)),\n",
      "(<시인, Score(score=6.211, frequency=17, branching_entropy=0.693, feature_fraction=0.500, eojeol_fraction=0.882)),\n",
      "(負, Score(score=6.211, frequency=3, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.000)),\n",
      "(<밤, Score(score=6.211, frequency=4, branching_entropy=0.562, feature_fraction=0.250, eojeol_fraction=0.000)),\n",
      "(각권, Score(score=6.211, frequency=17, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.941)),\n",
      "(개인과, Score(score=6.211, frequency=124, branching_entropy=0.562, feature_fraction=0.750, eojeol_fraction=0.968)),\n",
      "(자와, Score(score=6.211, frequency=82, branching_entropy=0.796, feature_fraction=0.714, eojeol_fraction=0.915)),\n",
      "(진료와, Score(score=6.211, frequency=11, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.909)),\n",
      "(로버트, Score(score=6.211, frequency=175, branching_entropy=0.562, feature_fraction=0.250, eojeol_fraction=0.977)),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "pretty_list(list(scores)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev: post-processing\n",
    "\n",
    "### branching entropy는 좀 구해볼까? \n",
    "    \n",
    "    L, right-side \n",
    "    R, left-side\n",
    "    \n",
    "    ## Complete\n",
    "    \n",
    "    >>> for w in ['대학', '대학생', '대학생과']:\n",
    "    >>>     print(w, '%.3f' % lentropy.get(w, 0))\n",
    "\n",
    "    대학 2.832\n",
    "    대학생 1.721\n",
    "    대학생과 0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_noun_score = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(score=1.104, frequency=67, branching_entropy=0.366, feature_fraction=0.761, eojeol_fraction=0.000)\n",
      "Score(score=2.978, frequency=64, branching_entropy=2.335, feature_fraction=0.739, eojeol_fraction=0.281)\n"
     ]
    }
   ],
   "source": [
    "score_dict = dict(scores)\n",
    "    \n",
    "pretty(score_dict['떡볶'])\n",
    "pretty(score_dict['떡볶이'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nsub + J: 떡볶 + 이\n",
    "\n",
    "    f(떡볶) ~= f(떡볶이): drop-rate, branching entropy로 하자\n",
    "    (떡볶이 in Noun) and (이 in Josa)\n",
    "    주로 한글자 짜리 조사가 문제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2274"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "josa = sorted(filter(lambda x:x[1] > 0.0001, coefficient.items()), key=lambda x:x[1], reverse=True)\n",
    "len(josa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [N + Jsub] + J: 대학생과 + 의\n",
    "\n",
    "    f(대학생) >> f(대학생과)\n",
    "    Right side branching entropy(대학생) ~= high\n",
    "    (과의 in Josa) and (대학생 in Noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound: 소수 + [집단 + 의]\n",
    "\n",
    "    소수 + 집단의 (집단의 = 집단 + 의 인지 확인)\n",
    "    Noun score 대체\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
