{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (15715, 4551)\n",
      "y shape = (15715,)\n",
      "# features = 4551\n",
      "# L words = 15715\n"
     ]
    }
   ],
   "source": [
    "head = 'l30_r15'\n",
    "directory = '../data/'\n",
    "model_fname ='../models/Logistic + L2 (C=1.00) norm l30_r15.pkl'\n",
    "\n",
    "\n",
    "import pickle    \n",
    "from py.utils import load_data\n",
    "\n",
    "x, y, x_words, vocabs = load_data(head, directory)\n",
    "with open(model_fname, 'rb') as f:\n",
    "    classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficient = {vocabs[j]:coef for j, coef in enumerate(classifier.coef_[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('어오고', -0.0035026965225787533),\n",
       " ('어놓는', -0.0017447214860182797),\n",
       " ('느냐고', -0.027019952035748179),\n",
       " ('라느니', 0.0013030948275454116),\n",
       " ('으셔서', -0.3228328145095205)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(coefficient.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from soynlp.utils import get_process_memory\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TrainedNounExtractor:\n",
    "    def __init__(self, coefficient, max_length=8):\n",
    "        self._coef = coefficient\n",
    "        self.lmax = max_length\n",
    "        \n",
    "    def extract(self, sents, min_count=10, min_noun_score=0.1):\n",
    "        self.lrgraph, self.lset, self.rset = self._build_lrgraph(sents, min_count)\n",
    "        self.lentropy, self.rentropy = self._branching_entropy(lrgraph)\n",
    "        scores = self._compute_noun_score(self.lrgraph)\n",
    "        return scores, self.lrgraph\n",
    "        scores = self._postprocessing(lrgraph, scores)\n",
    "\n",
    "    def _build_lrgraph(self, sents, min_count, pruning_min_count=2):\n",
    "        lset = {}\n",
    "        rset = {}\n",
    "        for n_sent, sent in enumerate(sents):\n",
    "            for eojeol in sent.split():\n",
    "                for e in range(1, min(len(eojeol), self.lmax)+1):\n",
    "                    l = eojeol[:e]\n",
    "                    r = eojeol[e:]\n",
    "                    lset[l] = lset.get(l, 0) + 1\n",
    "                    rset[r] = rset.get(r, 0) + 1\n",
    "            if n_sent % 1000 == 999:\n",
    "                args = (n_sent+1, len(lset), len(rset), get_process_memory())\n",
    "                sys.stdout.write('\\rscaning vocabulary ... %d sents #(l= %d, r= %d), mem= %.3f Gb' % args)\n",
    "            if n_sent % 500000 == 499999:\n",
    "                lset = {l:f for l,f in lset.items() if f >= pruning_min_count}\n",
    "                rset = {l:f for l,f in rset.items() if f >= pruning_min_count}\n",
    "        lset = {l:f for l,f in lset.items() if f >= min_count}\n",
    "        rset = {l:f for l,f in rset.items() if f >= min_count}\n",
    "        \n",
    "        n_sents = n_sent\n",
    "        \n",
    "        lrgraph = {}\n",
    "        for n_sent, sent in enumerate(sents):\n",
    "            for eojeol in sent.split():\n",
    "                for e in range(1, min(len(eojeol), self.lmax)+1):\n",
    "                    l = eojeol[:e]\n",
    "                    r = eojeol[e:]\n",
    "                    if not (l in lset) or not (r in rset):\n",
    "                        continue\n",
    "                    rdict = lrgraph.get(l, {})\n",
    "                    rdict[r] = rdict.get(r, 0) + 1\n",
    "                    lrgraph[l] = rdict            \n",
    "            if n_sent % 1000 == 999:\n",
    "                args = (100*(n_sent+1)/n_sents, '%', n_sent+1, n_sents, get_process_memory())\n",
    "                sys.stdout.write('\\rbuilding lrgraph ... (%.3f %s, %d in %d), mem= %.3f Gb' % args)\n",
    "        args = (len(lset), len(rset), sum((len(rdict) for rdict in lrgraph.values())), get_process_memory())\n",
    "        print('\\rlrgraph has been built. (#L= %d, #R= %d, #E=%d), mem= %.3f Gb' % args)\n",
    "        return lrgraph, lset, rset\n",
    "    \n",
    "    def _branching_entropy(self, lrgraph):\n",
    "        from collections import defaultdict\n",
    "        def entropy(d):\n",
    "            sum_ = sum(d.values())\n",
    "            if sum_ == 0: return 0\n",
    "            return -1 * sum((v/sum_) * np.log(v/sum_) for v in d.values())\n",
    "        def branch_map(d, get_branch):\n",
    "            b = defaultdict(lambda: 0)\n",
    "            for ext,f in d.items():\n",
    "                if ext != '':\n",
    "                    b[get_branch(ext)] += f\n",
    "            return b\n",
    "        def all_entropy(graph, get_branch=lambda x:x[0]):\n",
    "            return {w:entropy(branch_map(d, get_branch)) for w, d in graph.items()}\n",
    "        def to_rlgraph(lrgraph):\n",
    "            rlgraph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "            for l, rdict in lrgraph.items():\n",
    "                for r, f in rdict.items():\n",
    "                    rlgraph[r][l] += f\n",
    "            return {r:dict(ldict) for r, ldict in rlgraph.items()}    \n",
    "        print('compute branching entropy ...', end='')\n",
    "        lentropy = all_entropy(lrgraph)\n",
    "        rentropy = all_entropy(to_rlgraph(lrgraph), get_branch=lambda x:x[-1])\n",
    "        print(' done')\n",
    "        return lentropy, rentropy\n",
    "        \n",
    "    def _compute_noun_score(self, lrgraph):\n",
    "        from collections import namedtuple\n",
    "        Score = namedtuple('Score', 'score frequency branching_entropy feature_fraction eojeol_fraction')\n",
    "        scores = {}\n",
    "        n = len(lrgraph)\n",
    "        for i, (l, rdict) in enumerate(lrgraph.items()):\n",
    "            rdict_ = {r:f for r,f in rdict.items() if r in self._coef}\n",
    "            rsum = sum((f for r,f in rdict.items() if r != ''))\n",
    "            frequency = rsum + rdict.get('', 0)\n",
    "            feature_fraction = sum(rdict_.values()) / rsum if rsum > 0 else 0\n",
    "            eojeol_fraction = 1 - rsum / frequency\n",
    "            if not rdict_:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = sum(f*self._coef[r] for r, f in rdict_.items()) / sum(rdict_.values())\n",
    "            scores[l] = Score(score, frequency, self.lentropy.get(l, 0), feature_fraction, eojeol_fraction)\n",
    "            if (i+1) % 1000 == 0:\n",
    "                args = (100*(i+1)/n, '%', i+1, n)\n",
    "                sys.stdout.write('\\rcompute noun score ... (%.3f %s, %d in %d)' % args)\n",
    "        print('\\rcomputing noun score has been done.')\n",
    "        return sorted(scores.items(), key=lambda x:x[1].score, reverse=True)\n",
    "        \n",
    "    def _postprocessing(self, lrgraph, scores):\n",
    "        print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrgraph has been built. (#L= 201559, #R= 86324, #E=2487566), mem= 2.063 Gb\n",
      "compute branching entropy ... done\n",
      "computing noun score has been done.\n"
     ]
    }
   ],
   "source": [
    "from config import sentence_fname\n",
    "class Sentences:\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield doc.strip()\n",
    "\n",
    "sentences = Sentences(sentence_fname)\n",
    "noun_extractor = TrainedNounExtractor(coefficient)\n",
    "scores, lrgraph = noun_extractor.extract(sentences)\n",
    "\n",
    "lset, rset = noun_extractor.lset, noun_extractor.rset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_rlgraph(lrgraph):\n",
    "    from collections import defaultdict\n",
    "    rlgraph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for l, rdict in lrgraph.items():\n",
    "        for r, f in rdict.items():\n",
    "            rlgraph[r][l] += f\n",
    "    return {r:dict(ldict) for r, ldict in rlgraph.items()}\n",
    "\n",
    "rlgraph = to_rlgraph(lrgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor, Postprocessing의 필요성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lrgraph를 만들 때 한글 외의 기호를 삭제한 어절로 이뤄져야 함 \n",
    "- N = Nsub + J problem\n",
    "    - 중세보편주 + {의, 의의}\n",
    "- [N+Jsub] + J problem\n",
    "    - 대학생과 + {'', 의}\n",
    "    - {의, 과의} 모두 조사\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful function\n",
    "def pretty(namedtuple_instance, end='\\n'):\n",
    "    print('%s(%s)' % (namedtuple_instance.__class__.__name__, ', '.join(['%s=%d' % (field, value) if (type(value) == int or type(value) == np.int) else '%s=%.3f' % (field, value)  for field, value in namedtuple_instance._asdict().items()])),end=end )\n",
    "\n",
    "def pretty_list(l):\n",
    "    print('[', end='')\n",
    "    for w, nt in l[:10]:\n",
    "        print('(%s,'%w, end=' ')\n",
    "        pretty(nt, end='),\\n')\n",
    "    print(']' if len(l) <= 10 else '... ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'의': 6, '의를': 7, '의의': 2}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgraph['중세보편주']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 24, ',': 1, '의': 1}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgraph['대학생과']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 165),\n",
       " ('이', 74),\n",
       " ('들이', 73),\n",
       " ('들의', 67),\n",
       " ('들은', 36),\n",
       " ('의', 26),\n",
       " ('을', 26),\n",
       " ('과', 24),\n",
       " ('들을', 23),\n",
       " ('은', 21)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lrgraph['대학생'].items(), key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(개인과, Score(score=6.211, frequency=124, branching_entropy=0.562, feature_fraction=0.750, eojeol_fraction=0.968)),\n",
      "(로버트, Score(score=6.211, frequency=175, branching_entropy=0.562, feature_fraction=0.250, eojeol_fraction=0.977)),\n",
      "(자유주, Score(score=6.211, frequency=182, branching_entropy=-0.000, feature_fraction=0.148, eojeol_fraction=0.000)),\n",
      "(한낱, Score(score=6.211, frequency=127, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.984)),\n",
      "(우르, Score(score=6.211, frequency=158, branching_entropy=0.651, feature_fraction=0.006, eojeol_fraction=0.006)),\n",
      "(그런대로, Score(score=6.211, frequency=188, branching_entropy=0.693, feature_fraction=0.500, eojeol_fraction=0.989)),\n",
      "(당신과, Score(score=6.211, frequency=128, branching_entropy=0.349, feature_fraction=0.889, eojeol_fraction=0.930)),\n",
      "(낭만주, Score(score=6.211, frequency=179, branching_entropy=-0.000, feature_fraction=0.223, eojeol_fraction=0.000)),\n",
      "(여지껏, Score(score=6.211, frequency=106, branching_entropy=0.637, feature_fraction=0.333, eojeol_fraction=0.972)),\n",
      "(지금껏, Score(score=6.211, frequency=177, branching_entropy=0.693, feature_fraction=0.500, eojeol_fraction=0.989)),\n",
      "... ]\n"
     ]
    }
   ],
   "source": [
    "pretty_list(list(filter(lambda x:x[1].frequency > 100 and len(x[0]) > 1, scores))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(좌익과, Score(score=6.211, frequency=17, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.824)),\n",
      "(과거부터, Score(score=6.211, frequency=22, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.955)),\n",
      "(<시인, Score(score=6.211, frequency=17, branching_entropy=0.693, feature_fraction=0.500, eojeol_fraction=0.882)),\n",
      "(負, Score(score=6.211, frequency=3, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.000)),\n",
      "(<밤, Score(score=6.211, frequency=4, branching_entropy=0.562, feature_fraction=0.250, eojeol_fraction=0.000)),\n",
      "(각권, Score(score=6.211, frequency=17, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.941)),\n",
      "(개인과, Score(score=6.211, frequency=124, branching_entropy=0.562, feature_fraction=0.750, eojeol_fraction=0.968)),\n",
      "(자와, Score(score=6.211, frequency=82, branching_entropy=0.796, feature_fraction=0.714, eojeol_fraction=0.915)),\n",
      "(진료와, Score(score=6.211, frequency=11, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.909)),\n",
      "(로버트, Score(score=6.211, frequency=175, branching_entropy=0.562, feature_fraction=0.250, eojeol_fraction=0.977)),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "pretty_list(list(scores)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev: post-processing\n",
    "\n",
    "### branching entropy는 좀 구해볼까? \n",
    "    \n",
    "    L, right-side \n",
    "    R, left-side\n",
    "    \n",
    "    ## Complete\n",
    "    \n",
    "    >>> for w in ['대학', '대학생', '대학생과']:\n",
    "    >>>     print(w, '%.3f' % lentropy.get(w, 0))\n",
    "\n",
    "    대학 2.832\n",
    "    대학생 1.721\n",
    "    대학생과 0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_noun_score = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(score=1.104, frequency=67, branching_entropy=0.366, feature_fraction=0.761, eojeol_fraction=0.000)\n",
      "Score(score=2.978, frequency=64, branching_entropy=2.335, feature_fraction=0.739, eojeol_fraction=0.281)\n"
     ]
    }
   ],
   "source": [
    "score_dict = dict(scores)\n",
    "    \n",
    "pretty(score_dict['떡볶'])\n",
    "pretty(score_dict['떡볶이'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nsub + J: 떡볶 + 이\n",
    "\n",
    "    f(떡볶) ~= f(떡볶이): drop-rate, branching entropy로 하자\n",
    "    (떡볶이 in Noun) and (이 in Josa)\n",
    "    주로 한글자 짜리 조사가 문제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2375"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "josa = sorted(filter(lambda x:x[1] > 0, coefficient.items()), key=lambda x:x[1], reverse=True)\n",
    "len(josa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 는 정말 좋은 Regularization 인가? \n",
    "\n",
    "L2는 coefficients의 제곱의 합을 최소화 하려고 노력하기 때문에 자주 이용되는 features에 대한 coefficient의 절대값을 키우려는 경향이 있다. \n",
    "\n",
    "\"- 해줌으로써\"는 명사를 구분할 수 있는 좋은 조사이지만, 데이터에 등장한 횟수는 별로 많지 않다. 그렇기 때문에 coefficient가 매우 작게 나오게 된다. (coef = 0.000196)\n",
    "\n",
    "반대로 \"- 하는\"의 경우, [좋아 - 하는, 못 - 하는, ...]등의 오류가 있음에도 압도적인 명사들 때문에 높은 coefficient (2.033)을 가지게 된다. \n",
    "\n",
    "이런 입장에서 오히려 L2 regularization Logistic Regression 보다 Naive Bayes나 proportion 기반 방법이 더 좋은 것이 아닌가? \n",
    "\n",
    "L2 regularization은 학습데이터에서 등장한 R set의 frequency에 scale의 영향을 받는다. 본질적인 이유는 L-R matrix가 sparse matrix여서 L을 설명하는 R set features가 언제나 이용되는 것이 아니며, R set frequency 역시 Zipw's law를 따라 skewed 되어있기 때문이다. 그리고 데이터셋이 바뀜에 따라 R set frequency distribution이 달라지면 coefficient scale이 문제가 될 수 있다. 세종말뭉치에 이용되었던 문서들과 현제의 온라인 문서에서의 R set distribution은 분명 다를 수 밖에 없고, 지금 질 좋은 features가 될 수 있는 R set 들의 coefficients의 절대값이 작다면 분석이 잘 되지 않을 수도 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('보장', 2), ('마련', 2), ('보상', 2), ('명시', 1), ('보전', 1)] - 해줌으로써\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[('보장', 2), ('마련', 2), ('보상', 2), ('명시', 1), ('보전', 1), ('지원', 1), ('얘기', 1), ('지지', 1), ('제공', 1), ('대출', 1), ('인정', 1), ('대여', 1), ('확인', 1)]\""
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = '해줌으로써'\n",
    "print(str(sorted(rlgraph.get(w, {}).items(), key=lambda x:x[1], reverse=True)[:5]) + ' - %s' % w)\n",
    "str(sorted(rlgraph.get(w, {}).items(), key=lambda x:x[1], reverse=True)[:20])[:-1] + (']' if len(rlgraph.get(w, {})) <= 20 else ', ...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('못', 2114), ('말', 1953), ('생각', 1356), ('좋아', 1002), ('원', 847)] - 하는\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[('못', 2114), ('말', 1953), ('생각', 1356), ('좋아', 1002), ('원', 847), ('사용', 834), ('사랑', 673), ('일', 647), ('요구', 598), ('존재', 551), ('추구', 528), ('주장', 502), ('해당', 501), ('발생', 487), ('차지', 484), ('제공', 481), ('등장', 393), ('이해', 383), ('의미', 376), ('시작', 374), ...]\""
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = '하는'\n",
    "print(str(sorted(rlgraph.get(w, {}).items(), key=lambda x:x[1], reverse=True)[:5]) + ' - %s' % w)\n",
    "str(sorted(rlgraph.get(w, {}).items(), key=lambda x:x[1], reverse=True)[:20])[:-1] + (']' if len(rlgraph.get(w, {})) <= 20 else ', ...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0333080567632096, 0.00019593425186685041)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient['하는'], coefficient['해줌으로써'], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('의', 6.2108250932929217),\n",
       " ('를', 5.2275917393592097),\n",
       " ('가', 4.3986751160523632),\n",
       " ('한', 3.7657194150629776),\n",
       " ('에', 3.5512693551084609),\n",
       " ('로', 3.5400023136261112),\n",
       " ('이', 3.2809566440846751),\n",
       " ('을', 2.8668114740415227),\n",
       " ('으로', 2.6465314028770761),\n",
       " ('적인', 2.4873089663273582),\n",
       " ('하고', 2.3397713254276393),\n",
       " ('된', 2.1779089976672568),\n",
       " ('에서', 2.1476524916015003),\n",
       " ('적', 2.1083448239349738),\n",
       " ('하는', 2.0333080567632096),\n",
       " ('할', 2.0317829273285861),\n",
       " ('적으로', 1.6583744760803698),\n",
       " ('이다', 1.5285098649962987),\n",
       " ('들은', 1.5025060153070737),\n",
       " ('에게', 1.4847704268894344)]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "josa[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('됐음을', 0.00020547991455129269),\n",
       " ('끓는', 0.00020398189911457259),\n",
       " ('냐구요', 0.00020381950100549124),\n",
       " ('이었구나', 0.00020337780768687254),\n",
       " ('인가라고', 0.00020283424143817397),\n",
       " ('했느냐는', 0.00020240669293992069),\n",
       " ('에서두', 0.00020064144022782202),\n",
       " ('했단다', 0.00020015317375592496),\n",
       " ('적자금을', 0.00019608633897839539),\n",
       " ('해줌으로써', 0.00019593425186685041),\n",
       " ('에서만은', 0.00019590588147109036),\n",
       " ('달러', 0.00019551343979462479),\n",
       " ('해줘', 0.00019425354394409135),\n",
       " ('하셨고', 0.00019384787685696134),\n",
       " ('싶은', 0.00018758541793060106),\n",
       " ('했었지', 0.00018672308447793918),\n",
       " ('이었음은', 0.00018470679938860485),\n",
       " ('하셨는데', 0.00018294591009844179),\n",
       " ('하니까요', 0.00018007395979813116),\n",
       " ('이서만', 0.00017946260198717647)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "josa[-200:-180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [N + Jsub] + J: 대학생과 + 의\n",
    "\n",
    "    f(대학생) >> f(대학생과)\n",
    "    Right side branching entropy(대학생) ~= high\n",
    "    (과의 in Josa) and (대학생 in Noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound: 소수 + [집단 + 의]\n",
    "\n",
    "    소수 + 집단의 (집단의 = 집단 + 의 인지 확인)\n",
    "    Noun score 대체\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
