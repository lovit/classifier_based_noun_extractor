{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## lrdb.csv to sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5696420, 5696420, 5696420)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrdb_fname = '../data/lrdb.csv'\n",
    "\n",
    "def load(fname):\n",
    "    def remain_complete_korean(r):\n",
    "        kor_begin = 44032\n",
    "        kor_end = 55203\n",
    "        r = ''.join([ri for ri in r if kor_begin <= ord(ri) <= kor_end])\n",
    "        return r\n",
    "    def preprocessing(doc):\n",
    "        return (doc[0], doc[1], remain_complete_korean(doc[2]), doc[3], doc[4])\n",
    "    \n",
    "    # <eojeol, l, r, tag, l_stemmed>\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        docs = [doc.strip().split('\\t') for doc in f]\n",
    "    docs = [preprocessing(doc) for doc in docs if remain_complete_korean(doc[2])]\n",
    "    eojeol, l, r, tag, l0 = zip(*docs)\n",
    "    return l, r, tag\n",
    "\n",
    "l,r,t = load(lrdb_fname)\n",
    "len(l), len(r), len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min freq (0,0)  : #L= 201336, #R = 20252\n",
      "min freq (5,5)  : #L= 50766, #R = 4995\n",
      "min freq (10,10): #L= 31802, #R = 3297\n",
      "min freq (30,15): #L= 15166, #R = 2617\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "ltag_count = Counter([(li,ti) for li, ti in zip(l, t)])\n",
    "r_count = Counter(r)\n",
    "\n",
    "print('min freq (0,0)  : #L= %d, #R = %d' % (len(ltag_count), len(r_count)))\n",
    "print('min freq (5,5)  : #L= %d, #R = %d' % (len({li:f for li, f in ltag_count.items() if f >= 5}),\n",
    "                                        len({ri:f for ri, f in r_count.items() if f >= 5})))\n",
    "print('min freq (10,10): #L= %d, #R = %d' % (len({li:f for li, f in ltag_count.items() if f >= 10}),\n",
    "                                        len({ri:f for ri, f in r_count.items() if f >= 10})))\n",
    "print('min freq (30,15): #L= %d, #R = %d' % (len({li:f for li, f in ltag_count.items() if f >= 30}),\n",
    "                                        len({ri:f for ri, f in r_count.items() if f >= 15})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_train_data(head, directory, lset, rset):\n",
    "    d = {}\n",
    "    for li, ri, ti in zip(l, r, t):\n",
    "        key = (li, ti)\n",
    "        if not (key in lset) or not (ri in rset):\n",
    "            continue\n",
    "        features = d.get(key, {})\n",
    "        features[ri] = features.get(ri, 0) + 1\n",
    "        d[key] = features\n",
    "    \n",
    "    x_word = []\n",
    "    ylabel = []\n",
    "    vocabs = {ri:j for j, ri in enumerate(rset)}\n",
    "    \n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    \n",
    "    for (li, ti), rdict in d.items():\n",
    "        features = {vocabs[ri]:v for ri, v in rdict.items() if ri in vocabs}\n",
    "        if not features: continue\n",
    "        i = len(x_word)\n",
    "        x_word.append(li)\n",
    "        ylabel.append(ti)        \n",
    "        \n",
    "        for j, v in features.items():\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(v)\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    from scipy.sparse import csr_matrix\n",
    "    x = csr_matrix((data, (rows, cols)))\n",
    "    \n",
    "    from scipy.io import mmwrite\n",
    "    mm_fname = '%s/%s_x.mtx' % (directory, head)\n",
    "    mmwrite(mm_fname, x)\n",
    "    \n",
    "    x_word_fname = '%s/%s_x_word' % (directory, head)\n",
    "    with open(x_word_fname, 'w', encoding='utf-8') as f:\n",
    "        for word in x_word:\n",
    "            f.write('%s\\n' % word)\n",
    "    \n",
    "    x_word_fname = '%s/%s_y' % (directory, head)\n",
    "    with open(x_word_fname, 'w', encoding='utf-8') as f:\n",
    "        for y in ylabel:\n",
    "            f.write('%s\\n' % y)\n",
    "            \n",
    "    vocabs_fname = '%s/%s_vocabs' % (directory, head)\n",
    "    with open(vocabs_fname, 'w', encoding='utf-8') as f:\n",
    "        for ri in sorted(vocabs.keys(), key=lambda x:vocabs[x]):\n",
    "            f.write('%s\\n' % ri)\n",
    "            \n",
    "    return x, x_word, ylabel, vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50764, 4995), 50764, 4995, 50764)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lset = {li:f for li, f in ltag_count.items() if f >= 5}\n",
    "rset = {ri:f for ri, f in r_count.items() if f >= 5}\n",
    "x, x_word, ylabel, vocabs = create_train_data('l5_r5', '../data/', lset, rset)\n",
    "\n",
    "x.shape, len(ylabel), len(vocabs), len(x_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31797, 3297), 31797, 3297, 31797)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lset = {li:f for li, f in ltag_count.items() if f >= 10}\n",
    "rset = {ri:f for ri, f in r_count.items() if f >= 10}\n",
    "x, x_word, ylabel, vocabs = create_train_data('l10_r10', '../data/', lset, rset)\n",
    "\n",
    "x.shape, len(ylabel), len(vocabs), len(x_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15166, 2617), 15166, 2617, 15166)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lset = {li:f for li, f in ltag_count.items() if f >= 30}\n",
    "rset = {ri:f for ri, f in r_count.items() if f >= 15}\n",
    "x, x_word, ylabel, vocabs = create_train_data('l30_r15', '../data/', lset, rset)\n",
    "\n",
    "x.shape, len(ylabel), len(vocabs), len(x_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
