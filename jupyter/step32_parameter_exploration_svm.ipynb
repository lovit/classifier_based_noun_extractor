{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반화 성능도 있기 때문에 전체 데이터셋에 대해서 성능 평가를 해봐야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (15166, 2617)\n",
      "y shape = (15166,)\n",
      "# features = 2617\n",
      "# L words = 15166\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../models/SVC (C=0.1) norml30_r15.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "from py.utils import load_data\n",
    "head = 'l30_r15'\n",
    "directory = '../data/'\n",
    "x, y, x_words, vocabs = load_data(head, directory)\n",
    "x = x.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 52,  18,  16, ...,  22,  75, 234]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sample_r(li, topk=5):\n",
    "    nonzero = x[li,:].nonzero()[1]\n",
    "    base = min(50, len(nonzero)//2)\n",
    "    return [vocabs[r] for r in sorted(nonzero, key=lambda x:word_frequency[0,x], reverse=True)[base:base+topk]]\n",
    "\n",
    "word_frequency = x.sum(axis=0)\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5012)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# sparse matrix type\n",
    "print(classifier.dual_coef_.shape)\n",
    "\n",
    "# sum = 0 ?? alpha sum은 1아닌가?\n",
    "print(classifier.dual_coef_.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5012, 2617)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support vectors\n",
    "classifier.support_vectors_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33047606488197284"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of Support vector \n",
    "classifier.n_support_.sum() / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    5,     9,    13, ..., 15138, 15150, 15151], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector row id\n",
    "classifier.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피나(다) - ['게'] (-0.100)\n",
      "되묻(다) - ['는데', '기도', '자', '더니'] (-0.100)\n",
      "버티(다) - ['기는', '면서도', '듯이', '었기', '었지만'] (-0.100)\n",
      "빠져나오(다) - ['지도', '려고', '려는', '듯', '기는'] (-0.100)\n",
      "귀찮(다) - ['단', '으니까', '겠지만', '은데', '으면서도'] (-0.100)\n",
      "미치(다) - ['곤', '기가', '겠다고', '긴', '겠지만'] (-0.100)\n",
      "가진(다) - ['만큼', '다거나', '다든지', '데다', '대요'] (-0.100)\n",
      "사로잡(다) - ['았던', '으면서', '기에', '는다는', '겠다는'] (-0.100)\n",
      "버려(다) - ['지기', '두고', '지게', '졌던', '대는'] (-0.100)\n",
      "부릅뜨(다) - ['니', '고는', '니까', '신', '고서'] (-0.100)\n",
      "두드러지(다) - ['기', '던', '며', '면서', '지만'] (-0.100)\n",
      "드리(다) - ['겠다는', '듯이', '곤', '기가', '기만'] (-0.100)\n",
      "폭넓(다) - ['지', '지는', '고도', '음', '음에'] (-0.100)\n",
      "어지럽(다) - ['도록', '기도', '지도', '더니', '기만'] (-0.100)\n",
      "내보이(다) - ['기도', '지는', '니까', '기만', '려'] (-0.100)\n",
      "미쳐(다) - ['가고', '보는', '버릴', '버리고', '보지'] (-0.100)\n",
      "하찮(다) - ['게', '아', '기조차'] (-0.100)\n",
      "늘어났(다) - ['는데', '으며', '는지', '으나', '음을'] (-0.100)\n",
      "저러(다) - ['진', '지도', '는지', '면서도', '든지'] (-0.100)\n",
      "애써(다) - ['온', '야만', '보지', '왔지만', '줘서'] (-0.100)\n",
      "끌어당기(다) - ['자', '듯', '면서도', '고서', '느라'] (-0.100)\n",
      "꺼(다) - ['야지', '보는', '버리고', '지기', '두고'] (-0.100)\n",
      "들어가(다) - ['든지', '려', '든', '겠다고', '고자'] (-0.100)\n",
      "헤어지(다) - ['거나', '구', '지도', '려고', '더라도'] (-0.100)\n",
      "취하(다) - ['려', '든', '겠다고', '고자', '고도'] (-0.100)\n",
      "이글거리(다) - ['며', '기도', '면서부터'] (-0.100)\n",
      "계시(다) - ['더니', '기에', '듯이', '곤', '기만'] (-0.100)\n",
      "심하(다) - ['더니', '기는', '면서도', '긴', '신'] (-0.100)\n",
      "뻔하(다) - ['니', '거나', '니까', '긴', '단'] (-0.100)\n",
      "오르(다) - ['려', '겠다고', '고도', '긴', '려면'] (-0.100)\n",
      "싶(다) - ['으니까', '을까', '어지는', '겠지만', '게도'] (-0.100)\n",
      "난(다) - ['다는데', '다니까', '다던', '다고들', '다더니'] (-0.100)\n",
      "보태(다) - ['어도', '어진', '겠다는', '주는', '려'] (-0.100)\n",
      "달아나(다) - ['기에', '기는', '듯이', '곤', '기가'] (-0.100)\n",
      "아쉬워하(다) - ['며', '지만', '듯', '곤', '시고는'] (-0.100)\n",
      "쓰러뜨리(다) - ['기도', '거나', '지는', '려고', '기는'] (-0.100)\n",
      "짚(다) - ['어주는', '어낸', '어놓고', '어야만', '였을'] (-0.100)\n",
      "놓였(다) - ['으나', '었고', '었는데', '냐고', '다니'] (-0.100)\n",
      "무디(다) - ['어지는', '어지고', '어질', '어지지', '어지기'] (-0.100)\n",
      "지(다) - ['기는', '면서도', '었다고', '겠다는', '듯이'] (-0.100)\n"
     ]
    }
   ],
   "source": [
    "base = 0\n",
    "topk = 40\n",
    "for idx, l in sorted(enumerate(classifier.support_), key=lambda x:abs(classifier.dual_coef_[0,x[0]]), reverse=True)[base :base + topk]:\n",
    "    print('%s%s - %s (%.3f)' % (x_words[l], '' if y[l] == 1 else '(다)', get_sample_r(l), classifier.dual_coef_[0,idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of alphas = 2\n",
      "number of support vectors = 5012\n",
      "\n",
      "alpha= -0.100, count=2506 (0.500)\n",
      "alpha= 0.100, count=2506 (0.500)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "alpha_count = Counter(classifier.dual_coef_.data)\n",
    "print('number of alphas = %d' % len(alpha_count))\n",
    "print('number of support vectors = %d\\n' % len(classifier.dual_coef_.data))\n",
    "\n",
    "for alpha, count in sorted(alpha_count.items(), key=lambda x:(x[1], abs(x[0])), reverse=True)[:50]:\n",
    "    print('alpha= %.3f, count=%d (%.3f)' % (alpha, count, count/classifier.n_support_.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2506, 2506], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.n_support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Support Vector Machine은 약 30%의 매우 높은 support vector를 모델에 저장하고 있으며, 각각 support vector의 가중치 역시 다르지 않다. negative는 모두 -0.1, positive는 모두 0.1 weight를 가지고 있다. k-NN classifier와 다르지 않을 정도의 support vector ratio이며, 질 좋은 support vector를 선택하지도 (equal alpha) 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularization을 약하게 하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (15166, 2617)\n",
      "y shape = (15166,)\n",
      "# features = 2617\n",
      "# L words = 15166\n"
     ]
    }
   ],
   "source": [
    "with open('../models/Support Vector Machine (rbf, C=10.0) norml30_r15.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "from py.utils import load_data\n",
    "head = 'l30_r15'\n",
    "directory = '../data/'\n",
    "x, y, x_words, vocabs = load_data(head, directory)\n",
    "x = x.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20440458921271265"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of Support vector \n",
    "classifier.n_support_.sum() / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of alphas = 22\n",
      "number of support vectors = 3100\n",
      "\n",
      "alpha= -10.000, count=1542 (0.497)\n",
      "alpha= 10.000, count=1538 (0.496)\n",
      "alpha= 9.566, count=1 (0.000)\n",
      "alpha= 7.743, count=1 (0.000)\n",
      "alpha= 7.300, count=1 (0.000)\n",
      "alpha= 7.253, count=1 (0.000)\n",
      "alpha= 6.767, count=1 (0.000)\n",
      "alpha= -5.719, count=1 (0.000)\n",
      "alpha= -5.715, count=1 (0.000)\n",
      "alpha= 4.977, count=1 (0.000)\n",
      "alpha= 4.967, count=1 (0.000)\n",
      "alpha= 4.852, count=1 (0.000)\n",
      "alpha= -4.852, count=1 (0.000)\n",
      "alpha= -4.775, count=1 (0.000)\n",
      "alpha= 4.195, count=1 (0.000)\n",
      "alpha= -3.559, count=1 (0.000)\n",
      "alpha= 2.747, count=1 (0.000)\n",
      "alpha= 2.700, count=1 (0.000)\n",
      "alpha= 2.577, count=1 (0.000)\n",
      "alpha= 2.156, count=1 (0.000)\n",
      "alpha= -2.129, count=1 (0.000)\n",
      "alpha= -1.052, count=1 (0.000)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "alpha_count = Counter(classifier.dual_coef_.data)\n",
    "print('number of alphas = %d' % len(alpha_count))\n",
    "print('number of support vectors = %d\\n' % len(classifier.dual_coef_.data))\n",
    "\n",
    "for alpha, count in sorted(alpha_count.items(), key=lambda x:(x[1], abs(x[0])), reverse=True)[:50]:\n",
    "    print('alpha= %.3f, count=%d (%.3f)' % (alpha, count, count/classifier.n_support_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization 을 약하게 걸어도 대부분은 -10, 10의 weight를 가지며, support vector ratio가 20%로 줄어들었지만 여전히 많은 숫자이다. instance learning과 다르지 않다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
