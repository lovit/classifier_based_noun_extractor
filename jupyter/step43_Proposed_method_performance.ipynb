{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (15106, 2770)\n",
      "y shape = (15106,)\n",
      "# features = 2770\n",
      "# L words = 15106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1054566, 1054566)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = 'l30_r15'\n",
    "directory = '../data/'\n",
    "model_fname ='../models/Logistic + L2 (C=1.00) norm l30_r15.pkl'\n",
    "\n",
    "\n",
    "import pickle    \n",
    "from py.utils import load_data, Sentences\n",
    "from py.noun import TrainedNounExtractor\n",
    "from config import sentence_fname, sentence_tagged_fname\n",
    "\n",
    "X, y, x_words, vocabs = load_data(head, directory)\n",
    "with open(model_fname, 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "def load(fname, tagged=True):\n",
    "    split = lambda doc: doc.replace('//','').strip().split()\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = [[eojeol.split('+') if tagged else eojeol for eojeol in split(doc)] for doc in f]\n",
    "    return docs\n",
    "\n",
    "answers = load(sentence_tagged_fname)\n",
    "sentences = load(sentence_fname, False)\n",
    "\n",
    "len(answers), len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('사람', 54901),\n",
       " ('우리', 42698),\n",
       " ('생각', 31221),\n",
       " ('때문', 28192),\n",
       " ('그것', 25966),\n",
       " ('사회', 21304),\n",
       " ('문제', 18652),\n",
       " ('경우', 17210),\n",
       " ('하나', 16427),\n",
       " ('그녀', 15750),\n",
       " ('자신', 15676),\n",
       " ('시간', 14118),\n",
       " ('자기', 13577),\n",
       " ('아이', 12964),\n",
       " ('시작', 12703),\n",
       " ('소리', 12399),\n",
       " ('세계', 12343),\n",
       " ('정도', 12121),\n",
       " ('인간', 11993),\n",
       " ('한국', 11751)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "answer_noun_counter = Counter((word.split('/')[0] for doc in answers for eojeol in doc for word in eojeol if '/N' in word and len(word.split('/')[0]) > 1 ) )\n",
    "answer_noun_counter = {word:freq for word, freq in answer_noun_counter.items() if freq >= 10}\n",
    "\n",
    "sorted(answer_noun_counter.items(), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrgraph has been built. (#L= 173620, #R= 71297, #E=2022472), mem= 4.394 Gb\n",
      "compute branching entropy ... done\n",
      "computing noun score has been done.\n",
      "n_candidates= 62448\n",
      "n_nouns after substring processing= 52152\n",
      "n_nouns after compound processing= 46248\n"
     ]
    }
   ],
   "source": [
    "coefficient = {vocabs[j]:coef for j, coef in enumerate(classifier.coef_[0])}\n",
    "noun_extractor = TrainedNounExtractor(coefficient)\n",
    "scores = noun_extractor.extract(Sentences('../data/sentences_onlyhangle.txt'), min_count=10, min_noun_score=0.1)\n",
    "scores_ = dict(filter(lambda x:type(x[1]) != tuple and x[1].p_eojeol < 0.9, scores.items()))\n",
    "compounds_ = dict(filter(lambda x:type(x[1]) == tuple, scores.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_lr(e, w, t):\n",
    "    from hangle import decompose, compose, jaum_begin, jaum_end\n",
    "    tag = t[0][0]\n",
    "    i = 0\n",
    "    n = len(t)\n",
    "    for i_, ti in enumerate(t):\n",
    "        if t[0][0] == 'N' and ti[0] == 'V':\n",
    "            break\n",
    "        if t[0][0] == 'V' and (ti == 'ETN' and len(w[i_]) == 1 and jaum_begin <= ord(w[i_][0]) <= jaum_end):\n",
    "            tag = 'N'\n",
    "            break\n",
    "        if not (ti[0] == 'N' or ti == 'XSN' or ti[:2] == 'VV' or ti[:2] == 'VA' or ti == 'XR'):\n",
    "            break\n",
    "        i = i_\n",
    "    lw = e[:len(''.join(w[:i+1]))]\n",
    "    r = e[len(lw):]\n",
    "    \n",
    "    # 아빤 = 아빠/N + ㄴ/J\n",
    "    # 갈꺼야 = 가/V + ㄹ/E + 꺼야/E\n",
    "    if (t[i][0] == 'N' or t[i][0] == 'V') and (i+1 < n) and (jaum_begin <= ord(w[i+1][0]) <= jaum_end):\n",
    "        last_l = decompose(lw[-1])\n",
    "        l0 = lw[:-1] + compose(last_l[0], last_l[1], ' ')\n",
    "        return lw, r, tag == 'N'\n",
    "\n",
    "    # 가? = 가/V + ㅏ/E + ?/S\n",
    "    # 먹었어 = 먹/V + 었어/E\n",
    "    return lw, r, tag.replace('X','N') == 'N'\n",
    "\n",
    "def eojeol_to_wt(eojeol):\n",
    "    w = [e.split('/')[0] for e in eojeol]\n",
    "    t = [e.split('/')[1] for e in eojeol]\n",
    "    return w,t\n",
    "\n",
    "def left_tokenize(eojeol, nouns, reverse=True):\n",
    "    for i in reversed(range(1, len(eojeol)+1)) if reverse else range(1, len(eojeol)+1):\n",
    "        l = eojeol[:i]\n",
    "        r = eojeol[i:]\n",
    "        if l in nouns:\n",
    "            return l, r, True\n",
    "    return eojeol, '', False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposed_accuracy(sentences, answers, scores, reverse=True):\n",
    "    import sys\n",
    "    print('#sentences= %d, #answers= %d' % (len(sentences), len(answers)))\n",
    "    if len(sentences) != len(answers):\n",
    "        raise ValueError('not equal length')\n",
    "    \n",
    "    n_errors = 0\n",
    "    n_apos_is_ppos_t = 0\n",
    "    n_apos_is_ppos_f = 0\n",
    "    n_apos_is_pneg = 0\n",
    "    n_aneg_is_ppos = 0\n",
    "    n_aneg_is_pneg = 0\n",
    "    stop = False\n",
    "    \n",
    "    for k, (s, a) in enumerate(zip(sentences, answers)):\n",
    "        if stop: break\n",
    "        if k % 100 == 0:\n",
    "            sys.stdout.write('\\r... %d in %d' % (k+1, len(answers)))\n",
    "        \n",
    "        for e, ai in zip(s, a):\n",
    "            try:\n",
    "                la, ra, a_is_Noun = to_lr(e, *eojeol_to_wt(ai))\n",
    "                lp, rp, p_is_Noun = left_tokenize(e, scores, reverse)\n",
    "                if not (la in answer_noun_counter):\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "#                 stop = Trueleft_tokenize\n",
    "                n_errors += 1\n",
    "                continue\n",
    "            \n",
    "            if a_is_Noun and p_is_Noun:\n",
    "                if la == lp: n_apos_is_ppos_t += 1\n",
    "                else: n_apos_is_ppos_f += 1\n",
    "            elif a_is_Noun and not p_is_Noun: n_apos_is_pneg += 1\n",
    "            elif not a_is_Noun and p_is_Noun: n_aneg_is_ppos += 1\n",
    "            else: n_aneg_is_pneg += 1\n",
    "    print('\\ndone')\n",
    "    \n",
    "    return n_errors, n_apos_is_ppos_t, n_apos_is_ppos_f, n_apos_is_pneg, n_aneg_is_ppos, n_aneg_is_pneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences= 1054566, #answers= 1054566\n",
      "... 1054501 in 1054566\n",
      "done\n",
      "a=True, p=True, diff str:  0.03309291627348113\n",
      "a=True, p=True, same str:  0.9597533212243158\n",
      "accuracy wo str:  0.9928462374977969\n",
      "accuracy w str:  0.9597533212243158\n",
      "recall wo str:  0.9913643301903533\n",
      "recall w str:  0.9583208079041766\n",
      "precision wo str:  0.9361807618708302\n",
      "precision w str:  0.9049765830167971\n"
     ]
    }
   ],
   "source": [
    "# score, reverse=True\n",
    "nouns = set(scores.keys())\n",
    "nouns.update(set(compounds_.keys()))\n",
    "n_errors, n_apos_is_ppos_t, n_apos_is_ppos_f, n_apos_is_pneg, n_aneg_is_ppos, n_aneg_is_pneg = proposed_accuracy(sentences, answers, nouns, True)\n",
    "print('a=True, p=True, diff str: ', n_apos_is_ppos_f / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('a=True, p=True, same str: ', n_apos_is_ppos_t / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('accuracy wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('accuracy w str: ', (n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('recall wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_apos_is_pneg))\n",
    "print('recall w str: ', n_apos_is_ppos_t / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_apos_is_pneg))\n",
    "print('precision wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_ppos))\n",
    "print('precision w str: ', (n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_ppos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences= 1054566, #answers= 1054566\n",
      "... 1054501 in 1054566\n",
      "done\n",
      "a=True, p=True, diff str:  0.035012448298133214\n",
      "a=True, p=True, same str:  0.9533350407321534\n",
      "accuracy wo str:  0.9883474890302866\n",
      "accuracy w str:  0.9533350407321534\n",
      "recall wo str:  0.990058683511878\n",
      "recall w str:  0.9549856157363041\n",
      "precision wo str:  0.9401283068862559\n",
      "precision w str:  0.9068240347513996\n"
     ]
    }
   ],
   "source": [
    "# score_, reverse=True\n",
    "nouns = set(scores_.keys())\n",
    "nouns.update(set(compounds_.keys()))\n",
    "n_errors, n_apos_is_ppos_t, n_apos_is_ppos_f, n_apos_is_pneg, n_aneg_is_ppos, n_aneg_is_pneg = proposed_accuracy(sentences, answers, nouns, True)\n",
    "print('a=True, p=True, diff str: ', n_apos_is_ppos_f / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('a=True, p=True, same str: ', n_apos_is_ppos_t / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('accuracy wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('accuracy w str: ', (n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('recall wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_apos_is_pneg))\n",
    "print('recall w str: ', n_apos_is_ppos_t / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_apos_is_pneg))\n",
    "print('precision wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_ppos))\n",
    "print('precision w str: ', (n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_ppos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences= 1054566, #answers= 1054566\n",
      "... 1054501 in 1054566\n",
      "done\n",
      "a=True, p=True, diff str:  0.6742100810074885\n",
      "a=True, p=True, same str:  0.31413740802279816\n",
      "accuracy wo str:  0.9883474890302866\n",
      "accuracy w str:  0.31413740802279816\n",
      "recall wo str:  0.990058683511878\n",
      "recall w str:  0.3146812958811034\n",
      "precision wo str:  0.9401283068862559\n",
      "precision w str:  0.2988113723280377\n"
     ]
    }
   ],
   "source": [
    "# score_, reverse=False\n",
    "nouns = set(scores_.keys())\n",
    "nouns.update(set(compounds_.keys()))\n",
    "n_errors, n_apos_is_ppos_t, n_apos_is_ppos_f, n_apos_is_pneg, n_aneg_is_ppos, n_aneg_is_pneg = proposed_accuracy(sentences, answers, nouns, False)\n",
    "print('a=True, p=True, diff str: ', n_apos_is_ppos_f / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('a=True, p=True, same str: ', n_apos_is_ppos_t / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('accuracy wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('accuracy w str: ', (n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_pneg))\n",
    "print('recall wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_apos_is_pneg))\n",
    "print('recall w str: ', n_apos_is_ppos_t / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_apos_is_pneg))\n",
    "print('precision wo str: ', (n_apos_is_ppos_f + n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_ppos))\n",
    "print('precision w str: ', (n_apos_is_ppos_t) / (n_apos_is_ppos_t + n_apos_is_ppos_f + n_aneg_is_ppos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
