{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter, Kkma, Hannanum\n",
    "from config import sentence_kkma_fname, sentence_hannanum_fname, sentence_twitter_fname\n",
    "from config import sentence_fname, sentence_tagged_fname\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/sentences_twitter.txt was done\n",
      "../data/sentences_kkma.txt was done\n",
      "../data/sentences_hannanum.txt was done\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    output_fnames = [sentence_twitter_fname, sentence_kkma_fname, sentence_hannanum_fname]\n",
    "    taggers = [Twitter(), Kkma(), Hannanum()]\n",
    "\n",
    "    for output_fname, tagger in zip(output_fnames, taggers):\n",
    "        with open(sentence_fname, encoding='utf-8') as fi:\n",
    "            with open(output_fname, 'w', encoding='utf-8') as fo:\n",
    "                for i, doc in enumerate(fi):\n",
    "                    doc = '  '.join(['+'.join(['%s/%s' % (t[0], t[1]) for t in tagger.pos(eojeol)]) for eojeol in doc.split()])\n",
    "                    fo.write('%s\\n' % doc)\n",
    "                    if (i+1) % 1000 == 0:\n",
    "                        sys.stdout.write('\\r%s ... %d' % (output_fname, (i+1)))\n",
    "        sys.stdout.write('\\r%s was done\\n' % output_fname)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054566, 1054566)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(fname, tagged=True):\n",
    "    split = lambda doc: doc.replace('//','').strip().split()\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = [[eojeol.split('+') if tagged else eojeol for eojeol in split(doc)] for doc in f]\n",
    "    return docs\n",
    "\n",
    "answers = load(sentence_tagged_fname)\n",
    "sentences = load(sentence_fname, False)\n",
    "\n",
    "len(answers), len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('사람', 54901),\n",
       " ('우리', 42698),\n",
       " ('생각', 31221),\n",
       " ('때문', 28192),\n",
       " ('그것', 25966),\n",
       " ('사회', 21304),\n",
       " ('문제', 18652),\n",
       " ('경우', 17210),\n",
       " ('하나', 16427),\n",
       " ('그녀', 15750),\n",
       " ('자신', 15676),\n",
       " ('시간', 14118),\n",
       " ('자기', 13577),\n",
       " ('아이', 12964),\n",
       " ('시작', 12703),\n",
       " ('소리', 12399),\n",
       " ('세계', 12343),\n",
       " ('정도', 12121),\n",
       " ('인간', 11993),\n",
       " ('한국', 11751)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "answer_noun_counter = Counter((word.split('/')[0] for doc in answers for eojeol in doc for word in eojeol if '/N' in word and len(word.split('/')[0]) > 1 ) )\n",
    "answer_noun_counter = {word:freq for word, freq in answer_noun_counter.items() if freq >= 10}\n",
    "\n",
    "sorted(answer_noun_counter.items(), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "…/UNC 삼/NR 단/NNB 사/NR 단/NNB+이/VCP+면/EC 사/NR 단/NNB+,/SP\n",
      "어/IC+,/SP\n",
      "거기/NP 있/VA+을/ETM 때/NNG+,/SP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['거기/NP'], ['있/VA', '을/ETM'], ['때/NNG', ',/SP']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(sentence_tagged_fname) as f:\n",
    "    for _ in range(3):\n",
    "        print(next(f).strip())\n",
    "        \n",
    "answers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "… 삼 단 사 단이면 사 단,\n",
      "어::,\n",
      "거기 있을 때::,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['거기', '있을', '때::,']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(sentence_fname) as f:\n",
    "    for _ in range(3):\n",
    "        print(next(f).strip())\n",
    "\n",
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_lr(e, w, t):\n",
    "    from hangle import decompose, compose, jaum_begin, jaum_end\n",
    "    tag = t[0][0]\n",
    "    i = 0\n",
    "    n = len(t)\n",
    "    for i_, ti in enumerate(t):\n",
    "        if t[0][0] == 'N' and ti[0] == 'V':\n",
    "            break\n",
    "        if t[0][0] == 'V' and (ti == 'ETN' and len(w[i_]) == 1 and jaum_begin <= ord(w[i_][0]) <= jaum_end):\n",
    "            tag = 'N'\n",
    "            break\n",
    "        if not (ti[0] == 'N' or ti == 'XSN' or ti[:2] == 'VV' or ti[:2] == 'VA' or ti == 'XR'):\n",
    "            break\n",
    "        i = i_\n",
    "    lw = e[:len(''.join(w[:i+1]))]\n",
    "    r = e[len(lw):]\n",
    "    \n",
    "    # 아빤 = 아빠/N + ㄴ/J\n",
    "    # 갈꺼야 = 가/V + ㄹ/E + 꺼야/E\n",
    "    if (t[i][0] == 'N' or t[i][0] == 'V') and (i+1 < n) and (jaum_begin <= ord(w[i+1][0]) <= jaum_end):\n",
    "        last_l = decompose(lw[-1])\n",
    "        l0 = lw[:-1] + compose(last_l[0], last_l[1], ' ')\n",
    "        return lw, r, tag == 'N'\n",
    "\n",
    "    # 가? = 가/V + ㅏ/E + ?/S\n",
    "    # 먹었어 = 먹/V + 었어/E\n",
    "    return lw, r, tag.replace('X','N') == 'N'\n",
    "\n",
    "def eojeol_to_wt(eojeol):\n",
    "    w = [e.split('/')[0] for e in eojeol]\n",
    "    t = [e.split('/')[1] for e in eojeol]\n",
    "    return w,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('단', '이면', True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_lr('단이면', ['단', '이', '면'], ['NNB','VCP','EC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('단', '이면', True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_lr('단이면', *eojeol_to_wt(['단/NNB', '이/VCP', '면/EC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(sentences, answers, taggeds):\n",
    "    import sys\n",
    "    print('#sentences= %d, #answers= %d, #taggeds= %d' % (len(sentences), len(answers), len(taggeds)))\n",
    "    if not (len(sentences) == len(answers) and len(answers) == len(taggeds)):\n",
    "        raise ValueError('not equal length')\n",
    "    \n",
    "    n_errors = 0\n",
    "    n_apos_is_tpos_t = 0\n",
    "    n_apos_is_tpos_f = 0\n",
    "    n_apos_is_tneg = 0\n",
    "    n_aneg_is_tpos = 0\n",
    "    n_aneg_is_tneg = 0\n",
    "    stop = False\n",
    "    \n",
    "    for k, (s, a, t) in enumerate(zip(sentences, answers, taggeds)):\n",
    "        if stop: break\n",
    "        if k % 100 == 0:\n",
    "            sys.stdout.write('\\r... %d in %d' % (k+1, len(answers)))\n",
    "            \n",
    "        if not (len(s) == len(a) and len(a) == len(t)):\n",
    "            n_errors += 1\n",
    "            continue\n",
    "        for e, ai, ti in zip(s, a, t):\n",
    "            try:\n",
    "                la, ra, a_is_Noun = to_lr(e, *eojeol_to_wt(ai))\n",
    "                lt, rt, t_is_Noun = to_lr(e, *eojeol_to_wt(ti))\n",
    "                if not (la in answer_noun_counter):\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "#                 stop = True\n",
    "                n_errors += 1\n",
    "                continue\n",
    "            \n",
    "            if a_is_Noun and t_is_Noun:\n",
    "                if la == lt: n_apos_is_tpos_t += 1\n",
    "                else: n_apos_is_tpos_f += 1\n",
    "            elif a_is_Noun and not t_is_Noun: n_apos_is_tneg += 1\n",
    "            elif not a_is_Noun and t_is_Noun: n_aneg_is_tpos += 1\n",
    "            else: n_aneg_is_tneg += 1\n",
    "    \n",
    "    return n_errors, n_apos_is_tpos_t, n_apos_is_tpos_f, n_apos_is_tneg, n_aneg_is_tpos, n_aneg_is_tneg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter\n",
      "#sentences= 1054566, #answers= 1054566, #taggeds= 1054566\n",
      "... 1054501 in 1054566\n",
      "\n",
      "kkma\n",
      "#sentences= 1054566, #answers= 1054566, #taggeds= 1054566\n",
      "... 1054501 in 1054566\n",
      "\n",
      "hannanum\n",
      "#sentences= 1054566, #answers= 1054566, #taggeds= 1054566\n",
      "... 1054501 in 1054566"
     ]
    }
   ],
   "source": [
    "performances = {}\n",
    "print('twitter')\n",
    "performances['twitter'] = accuracy(sentences, answers, load(sentence_twitter_fname))\n",
    "\n",
    "print('\\n\\nkkma')\n",
    "performances['kkma'] = accuracy(sentences, answers, load(sentence_kkma_fname))\n",
    "\n",
    "print('\\n\\nhannanum')\n",
    "performances['hannanum'] = accuracy(sentences, answers, load(sentence_hannanum_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hannanum': (11387, 3683456, 347378, 76220, 84957, 221331),\n",
       " 'kkma': (13046, 3956310, 82752, 67536, 45493, 260714),\n",
       " 'twitter': (12021, 3587705, 136442, 382898, 235123, 71090)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# tagger= kkma\n",
      "a=True, p=True, diff str:  0.019245653727077875\n",
      "a=True, p=True, same str:  0.9201200248571089\n",
      "accuracy wo str:  0.9393656785841867\n",
      "accuracy w str:  0.9201200248571089\n",
      "recall wo str:  0.9835542704691328\n",
      "recall w str:  0.9634032841782907\n",
      "precision wo str:  0.9888621893939487\n",
      "precision w str:  0.9686024548573835\n",
      "\n",
      "\n",
      "# tagger= twitter\n",
      "a=True, p=True, diff str:  0.03595085102722175\n",
      "a=True, p=True, same str:  0.9453177759386304\n",
      "accuracy wo str:  0.9812686269658522\n",
      "accuracy w str:  0.9453177759386304\n",
      "recall wo str:  0.9067704395739515\n",
      "recall w str:  0.8735489871671726\n",
      "precision wo str:  0.9406145577341277\n",
      "precision w str:  0.9061531544956519\n",
      "\n",
      "\n",
      "# tagger= hannanum\n",
      "a=True, p=True, diff str:  0.08169438391972089\n",
      "a=True, p=True, same str:  0.8662542493059417\n",
      "accuracy wo str:  0.9479486332256627\n",
      "accuracy w str:  0.8662542493059417\n",
      "recall wo str:  0.9814416854514209\n",
      "recall w str:  0.8968608642593937\n",
      "precision wo str:  0.9793582813121463\n",
      "precision w str:  0.894957008264025\n"
     ]
    }
   ],
   "source": [
    "for tagger_name, performance in performances.items():\n",
    "    print('\\n\\n# tagger= %s' % tagger_name)\n",
    "    \n",
    "    n_errors, n_apos_is_tpos_t, n_apos_is_tpos_f, n_apos_is_tneg, n_aneg_is_tpos, n_aneg_is_tneg = performance\n",
    "    \n",
    "    print('a=True, p=True, diff str: ', n_apos_is_tpos_f / (n_apos_is_tpos_t + n_apos_is_tpos_f + n_aneg_is_tneg))\n",
    "    print('a=True, p=True, same str: ', n_apos_is_tpos_t / (n_apos_is_tpos_t + n_apos_is_tpos_f + n_aneg_is_tneg))\n",
    "    print('accuracy wo str: ', (n_apos_is_tpos_f + n_apos_is_tpos_t) / (n_apos_is_tpos_t + n_apos_is_tpos_f + n_aneg_is_tneg))\n",
    "    print('accuracy w str: ', (n_apos_is_tpos_t) / (n_apos_is_tpos_t + n_apos_is_tpos_f + n_aneg_is_tneg))\n",
    "    print('recall wo str: ', (n_apos_is_tpos_f + n_apos_is_tpos_t) / (n_apos_is_tpos_t + n_apos_is_tpos_f + n_apos_is_tneg))\n",
    "    print('recall w str: ', n_apos_is_tpos_t / (n_apos_is_tpos_t + n_apos_is_tpos_f + n_apos_is_tneg))\n",
    "    print('precision wo str: ', (n_apos_is_tpos_f + n_apos_is_tpos_t) / (n_apos_is_tpos_t + n_apos_is_tpos_f + n_aneg_is_tpos))\n",
    "    print('precision w str: ', (n_apos_is_tpos_t) / (n_apos_is_tpos_t + n_apos_is_tpos_f + n_aneg_is_tpos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.4'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
