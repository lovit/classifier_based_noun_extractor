{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (15166, 2617)\n",
      "y shape = (15166,)\n",
      "# features = 2617\n",
      "# L words = 15166\n"
     ]
    }
   ],
   "source": [
    "head = 'l30_r15'\n",
    "directory = '../data/'\n",
    "model_fname ='../models/Logistic + L2 (C=1.00) norm l30_r15.pkl'\n",
    "\n",
    "\n",
    "import pickle    \n",
    "from py.utils import load_data\n",
    "\n",
    "X, y, x_words, vocabs = load_data(head, directory)\n",
    "with open(model_fname, 'rb') as f:\n",
    "    classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficient = {vocabs[j]:coef for j, coef in enumerate(classifier.coef_[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('어놓는', -0.008046336849911781),\n",
       " ('어내야', -0.0037413806402452803),\n",
       " ('아낸', -0.044931991059519706),\n",
       " ('내린', -0.1041891005850004),\n",
       " ('대요', -0.003911649230608623)]"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(coefficient.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from soynlp.utils import get_process_memory\n",
    "from soynlp.hangle import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open(sentence_fname, encoding='utf-8') as f:\n",
    "#     with open('../data/sentences_onlyhangle.txt', 'w', encoding='utf-8') as fo:        \n",
    "#         for sent in f:\n",
    "#             fo.write('%s\\n' % normalize(sent.strip()))\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TrainedNounExtractor:\n",
    "    def __init__(self, coefficient, max_length=8):\n",
    "        self._coef = coefficient\n",
    "        self.lmax = max_length\n",
    "        \n",
    "    def extract(self, sents, min_count=10, min_noun_score=0.1):\n",
    "        self.lrgraph, self.lset, self.rset = self._build_lrgraph(sents, min_count)\n",
    "        self.lentropy, self.rentropy = self._branching_entropy(lrgraph)\n",
    "        scores = self._compute_noun_score(self.lrgraph)\n",
    "        return scores, self.lrgraph\n",
    "        scores = self._postprocessing(lrgraph, scores)\n",
    "\n",
    "    def _build_lrgraph(self, sents, min_count, pruning_min_count=2):\n",
    "        lset = {}\n",
    "        rset = {}\n",
    "        for n_sent, sent in enumerate(sents):\n",
    "            for eojeol in sent.split():\n",
    "                for e in range(1, min(len(eojeol), self.lmax)+1):\n",
    "                    l = eojeol[:e]\n",
    "                    r = eojeol[e:]\n",
    "                    lset[l] = lset.get(l, 0) + 1\n",
    "                    rset[r] = rset.get(r, 0) + 1\n",
    "            if n_sent % 1000 == 999:\n",
    "                args = (n_sent+1, len(lset), len(rset), get_process_memory())\n",
    "                sys.stdout.write('\\rscaning vocabulary ... %d sents #(l= %d, r= %d), mem= %.3f Gb' % args)\n",
    "            if n_sent % 500000 == 499999:\n",
    "                lset = {l:f for l,f in lset.items() if f >= pruning_min_count}\n",
    "                rset = {l:f for l,f in rset.items() if f >= pruning_min_count}\n",
    "        lset = {l:f for l,f in lset.items() if f >= min_count}\n",
    "        rset = {l:f for l,f in rset.items() if f >= min_count}\n",
    "        \n",
    "        n_sents = n_sent\n",
    "        \n",
    "        lrgraph = {}\n",
    "        for n_sent, sent in enumerate(sents):\n",
    "            for eojeol in sent.split():\n",
    "                for e in range(1, min(len(eojeol), self.lmax)+1):\n",
    "                    l = eojeol[:e]\n",
    "                    r = eojeol[e:]\n",
    "                    if not (l in lset) or not (r in rset):\n",
    "                        continue\n",
    "                    rdict = lrgraph.get(l, {})\n",
    "                    rdict[r] = rdict.get(r, 0) + 1\n",
    "                    lrgraph[l] = rdict            \n",
    "            if n_sent % 1000 == 999:\n",
    "                args = (100*(n_sent+1)/n_sents, '%', n_sent+1, n_sents, get_process_memory())\n",
    "                sys.stdout.write('\\rbuilding lrgraph ... (%.3f %s, %d in %d), mem= %.3f Gb' % args)\n",
    "        args = (len(lset), len(rset), sum((len(rdict) for rdict in lrgraph.values())), get_process_memory())\n",
    "        print('\\rlrgraph has been built. (#L= %d, #R= %d, #E=%d), mem= %.3f Gb' % args)\n",
    "        return lrgraph, lset, rset\n",
    "    \n",
    "    def _branching_entropy(self, lrgraph):\n",
    "        from collections import defaultdict\n",
    "        def entropy(d):\n",
    "            sum_ = sum(d.values())\n",
    "            if sum_ == 0: return 0\n",
    "            return -1 * sum((v/sum_) * np.log(v/sum_) for v in d.values())\n",
    "        def branch_map(d, get_branch):\n",
    "            b = defaultdict(lambda: 0)\n",
    "            for ext,f in d.items():\n",
    "                if ext != '':\n",
    "                    b[get_branch(ext)] += f\n",
    "            return b\n",
    "        def all_entropy(graph, get_branch=lambda x:x[0]):\n",
    "            return {w:entropy(branch_map(d, get_branch)) for w, d in graph.items()}\n",
    "        def to_rlgraph(lrgraph):\n",
    "            rlgraph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "            for l, rdict in lrgraph.items():\n",
    "                for r, f in rdict.items():\n",
    "                    if r == '': continue\n",
    "                    rlgraph[r][l] += f\n",
    "            return {r:dict(ldict) for r, ldict in rlgraph.items()}    \n",
    "        print('compute branching entropy ...', end='')\n",
    "        lentropy = all_entropy(lrgraph)\n",
    "        rentropy = all_entropy(to_rlgraph(lrgraph), get_branch=lambda x:x[-1])\n",
    "        print(' done')\n",
    "        return lentropy, rentropy\n",
    "        \n",
    "    def _compute_noun_score(self, lrgraph):\n",
    "        from collections import namedtuple\n",
    "        Score = namedtuple('Score', 'score frequency branching_entropy feature_fraction eojeol_fraction')\n",
    "        scores = {}\n",
    "        n = len(lrgraph)\n",
    "        for i, (l, rdict) in enumerate(lrgraph.items()):\n",
    "            rdict_ = {r:f for r,f in rdict.items() if r in self._coef}\n",
    "            rsum = sum((f for r,f in rdict.items() if r != ''))\n",
    "            frequency = self.lset.get(l, 0)\n",
    "            feature_fraction = sum(rdict_.values()) / rsum if rsum > 0 else 0\n",
    "            eojeol_fraction = rdict.get('', 0) / frequency\n",
    "            if not rdict_:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = sum(f*self._coef[r] for r, f in rdict_.items()) / sum(rdict_.values())\n",
    "            scores[l] = Score(score, frequency, self.lentropy.get(l, 0), feature_fraction, eojeol_fraction)\n",
    "            if (i+1) % 1000 == 0:\n",
    "                args = (100*(i+1)/n, '%', i+1, n)\n",
    "                sys.stdout.write('\\rcompute noun score ... (%.3f %s, %d in %d)' % args)\n",
    "        print('\\rcomputing noun score has been done.')\n",
    "        return scores\n",
    "#         return sorted(scores.items(), key=lambda x:x[1].score, reverse=True)\n",
    "        \n",
    "    def _postprocessing(self, lrgraph, scores):\n",
    "        print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrgraph has been built. (#L= 173620, #R= 71297, #E=2022472), mem= 2.099 Gb\n",
      "compute branching entropy ... done\n",
      "computing noun score has been done.\n"
     ]
    }
   ],
   "source": [
    "from config import sentence_fname\n",
    "class Sentences:\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield doc.strip()\n",
    "\n",
    "sentences = Sentences('../data/sentences_onlyhangle.txt')\n",
    "noun_extractor = TrainedNounExtractor(coefficient)\n",
    "scores, lrgraph = noun_extractor.extract(sentences)\n",
    "\n",
    "lset, rset = noun_extractor.lset, noun_extractor.rset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_rlgraph(lrgraph):\n",
    "    from collections import defaultdict\n",
    "    rlgraph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for l, rdict in lrgraph.items():\n",
    "        for r, f in rdict.items():\n",
    "            if r == '': continue\n",
    "            rlgraph[r][l] += f\n",
    "    return {r:dict(ldict) for r, ldict in rlgraph.items()}\n",
    "\n",
    "rlgraph = to_rlgraph(lrgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor, Postprocessing의 필요성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lrgraph를 만들 때 한글 외의 기호를 삭제한 어절로 이뤄져야 함 \n",
    "- N = Nsub + J problem\n",
    "    - 중세보편주 + {의, 의의}\n",
    "- [N+Jsub] + J problem\n",
    "    - 대학생과 + {'', 의}\n",
    "    - {의, 과의} 모두 조사\n",
    "- 1음절 명사들은 제외하였다. 필요하면 그 때 추가하도록. 아래는 0.5 를 넘은 1음절 명사들이다 \n",
    "\n",
    "    질 럼 듀 균 흑 첨 응 쌘 초 삥 휭 릴 변 문 핸 윈 심 팅 뉴 를 맏 갱 흥 국 결 잣 휴 콤 도 닉 퓌 쫌 넷 놋 깡 앨 캡 불 횡 완 샤 원 삽 당 비 총 서 시 튜 골 ㅅ 판 범 옥 꽤 껀 빈 란 장 셰 징 산 송 쎈 덴 쑥 쉼 냇 펑 택 밸 군 외 표 멍 볼 카 렌 평 딱 쟁 헛 웹 멧 귄 띵 탈 망 번 케 절 퍽 약 갯 깃 그 셋 피 캠 뿐 턴 롤 흔 겹 빅 철 간 촌 령 측 꽝 겸 논 암 뚝 욱 얀 뭣 취 껄 칭 때 면 쇄 쇳 안 잠 푼 종 말 쌈 차 성 침 길 의 칠 호 향 전 형 킴 흉 패 둘 엠 끝 양 넙 람 옴 악 통 싯 팝 수 좀 새 배 눔 쿵 채 회 똔 괄 림 윷 벱 발 벨 랑 날 일 분 쯔 찡 읍 콘 픽 편 헉 극 셈 햄 관 제 팽 또 팬 권 쾅 땐 흠 너 격 탕 뻑 훤 목 휠 병 휑 즤 후 김 톱 잔 숀 쿨 욕 탭 묘 젤 방 뻔 잼 것 누 맛 급 닌 갓 겉 척 끗 링 화 탓 멜 못 잭 내 곡 답 강 족 걔 법 뻥 킬 귤 년 뽁 중 애 득 책 셸 루 빤 죄 곰 똘 멱 꾀 뜻 궤 쟤 늬 록 놈 쿡 룰 폼 폭 옻 션 펠 플 윌 캔 브 갠 뜸 칩 홑 덕 앞 월 뚱 앳 섬 왕 힘 숄 론 글 돛 밥 맘 똥 키 대 식 위 맥 틱 점 떡 룸 빽 값 꼿 빛 폴 늠 색 쇠 움 봄 집 뼘 바 쯤 딸 씽 꽃 뭔 곳 첩 손 락 딕 껌 빵 톰 뿔 잽 껩 술 짬 획 숍 녘 냥 펙 앰 꿈 탑 숯 벽 씩 량 층 갭 잎 펌 겐 북 착 딘 팩 힐 펜 리 벼 곽 삯 프 티 핵 댁 홈 옷 칡 별 꿩 뇌 춤 예 턱 밤 뽕 쪽 짐 개 씨 삔 굿 꼴 콩 붐 팥 솜 샬 넋 칸 밭 쉿 휙 꾹 돈 칼 팀 니 듯 빔 샘 눈 랩 몫 펄 컵 둑 툴 틈 꿀 뱀 컷 쉘 씬 흙 팁 큐 훅 앎 늪 숲 속 흐 뫼 샅 돔 뒤 뼈 쌀 닭 코 곁 뺨 쨍 볏 샷 뽈 뮤 쟝 밑 괌 홀 괘 롬 짝 맴 명 빰 녜 싹 팸 퀭 험 땀 닢 덤 볕 슛 뜰 몸 땅 즙 킥 홉 닻 뭍 립 밖 솥 쑈 퀸 텔 덫 옆 쇼 퉤 쌍 댐 룩 으 램 땜 닐 귀 콕 톤 첼 등 숱 텀 샨 쿤 섶 력 꾐 윙 씰 훠 쩜 엥 콰 캉 팰 삶 님 킹 네 므 류 괜 톨 혀 홰 웜 녀 갸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful function\n",
    "def pretty(namedtuple_instance, end='\\n'):\n",
    "    print('%s(%s)' % (namedtuple_instance.__class__.__name__, ', '.join(['%s=%d' % (field, value) if (type(value) == int or type(value) == np.int) else '%s=%.3f' % (field, value)  for field, value in namedtuple_instance._asdict().items()])),end=end )\n",
    "\n",
    "def pretty_list(l):\n",
    "    print('[', end='')\n",
    "    for w, nt in l[:10]:\n",
    "        print('(%s,'%w, end=' ')\n",
    "        pretty(nt, end='),\\n')\n",
    "    print(']' if len(l) <= 10 else '... ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'의': 6, '의를': 7, '의의': 2}"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgraph['중세보편주']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 25, '의': 1}"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgraph['대학생과']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 206),\n",
       " ('이', 76),\n",
       " ('들이', 76),\n",
       " ('들의', 68),\n",
       " ('들은', 39),\n",
       " ('을', 29),\n",
       " ('의', 26),\n",
       " ('과', 25),\n",
       " ('들', 24),\n",
       " ('들을', 23)]"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lrgraph['대학생'].items(), key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(편협, Score(score=3.328, frequency=107, branching_entropy=1.224, feature_fraction=0.890, eojeol_fraction=0.056)),\n",
      "(따지, Score(score=-1.610, frequency=903, branching_entropy=2.038, feature_fraction=0.990, eojeol_fraction=0.013)),\n",
      "(한장, Score(score=2.965, frequency=120, branching_entropy=2.097, feature_fraction=0.813, eojeol_fraction=0.350)),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "pretty_list(list(filter(lambda x:x[1].frequency > 100 and len(x[0]) > 1, scores.items()))[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(교육여건, Score(score=3.340, frequency=23, branching_entropy=1.679, feature_fraction=0.867, eojeol_fraction=0.348)),\n",
      "(글공부, Score(score=3.227, frequency=14, branching_entropy=1.179, feature_fraction=1.000, eojeol_fraction=0.071)),\n",
      "(몰랐는, Score(score=-0.377, frequency=71, branching_entropy=0.074, feature_fraction=0.986, eojeol_fraction=0.000)),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "pretty_list(list(scores.items())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev: post-processing\n",
    "\n",
    "### branching entropy는 좀 구해볼까? \n",
    "    \n",
    "    L, right-side \n",
    "    R, left-side\n",
    "    \n",
    "    ## Complete\n",
    "    \n",
    "    >>> for w in ['대학', '대학생', '대학생과']:\n",
    "    >>>     print(w, '%.3f' % lentropy.get(w, 0))\n",
    "\n",
    "    대학 2.832\n",
    "    대학생 1.721\n",
    "    대학생과 0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_noun_score = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(score=1.943, frequency=77, branching_entropy=0.406, feature_fraction=0.620, eojeol_fraction=0.000)\n",
      "Score(score=3.215, frequency=67, branching_entropy=1.944, feature_fraction=0.900, eojeol_fraction=0.388)\n"
     ]
    }
   ],
   "source": [
    "score_dict = dict(scores)\n",
    "s_ = score_dict['떡볶']\n",
    "s = score_dict['떡볶이']\n",
    "pretty(s_)\n",
    "pretty(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nsub + J: 떡볶 + 이\n",
    "\n",
    "    f(떡볶) ~= f(떡볶이): drop-rate, branching entropy로 하자\n",
    "    (떡볶이 in Noun) and (이 in Josa)\n",
    "    주로 한글자 짜리 조사가 문제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2409"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "josa = sorted(filter(lambda x:x[1] > -0.1, coefficient.items()), key=lambda x:x[1], reverse=True)\n",
    "josa_dict = dict(josa)\n",
    "len(josa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'의 를 가 한 인 에 이 으로 로 을 된 하는 할 하고 에서 과 하게 에게 와 되는 되어 에는 만 부터 까지 해 하여 처럼 로운 이나 도 러 아야 이고 해야 스럽게 되고 치 해서 하지 될 듯이 롭게 롭지 하기 없이 이지 에서는 마다 라고 든지 이라도 시키는 듯 인지 보다 로는 하면 되지 으로는 같이 돼 정책 스런 스러운 자면 으니까 한테 하면서 나 시켜 이라고 하며 라도 이라는 엔 기를 일 기의 은 시키고 이면 라는 되어야 시킬 서도 에게는 이나마 여서 기는 까지나 았는데 였고 에서도 시킨 한다는 하던 기에 서만 만한 았다는 이던 기가 았다고 롭고 니까 이지만 되기 고자 이야 인데 하거나 다시피 되면 시키기 만의 에다 으러 된다는 까지는 워져'"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tuple(zip(*(filter(lambda x:x[1] > 0.1, josa))))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postnoun = set('가감계과권꾼네님대댁당들력로론류률상생성시식용율의인이일재적제중째쯤층치판풍형화')\n",
    "def find_composed_josa(josa):\n",
    "    offset = set('계권꾼네님대력론류상생성시식용율적째쯤층치풍형화')\n",
    "    return {j for j in josa if ((len(j) > 1) and (j[0] in josa) and not (j[0] in offset) and (j[1:] in josa))}\n",
    "\n",
    "composed_josa = find_composed_josa(tuple(zip(*josa))[0])\n",
    "jsub_set = {j[0] for j in composed_josa}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명사 뒤에 결합하여 의미를 확장하는 의존명사 (postnoun)이면서 조사에서는 잘 이용되지 않는 1음절 단어들을 offset으로 등록하였다. \n",
    "\n",
    "의존명사는 명사를 추출하는 질 좋은 R set feature이다. '나무 + 꾼' 처럼 '나무'가 명사라는 걸 잘 알려주기 때문이다. 하지만 실제로는 '나무꾼'을 단일 명사로 추출해야 한다. \n",
    "\n",
    "그 외에는 [-가, -이, -로, ...]의 경우 조사일 수도 있고, 다른 조사의 left-side 일수도 있다. 해당하는 조사들과 각각의 coefficient를 나열하면 아래와 같다. \n",
    "\n",
    "    >>> sorted({(j[0], '['+', '.join(tuple(zip(*sorted({'-'+j_[1:]:coef for j_,coef in coefficient.items() if len(j_) > 1 and j[0] == j_[0]}.items(), key=lambda x:x[1], reverse=True)))[0][:5])+']', '%.3f'%coefficient[j[0]]) for j in composed_josa}, key=lambda x:x[2], reverse=True)\n",
    "\n",
    "    [('가', '[-에, -가, -의, -에서, -로]', '4.399'),\n",
    "     ('한', '[-다, -테, -다는, -다고, -다면]', '3.766'),\n",
    "     ('에', '[-서, -게, -는, -서는, -게는]', '3.551'),\n",
    "     ('로', '[-운, -는, -서, -서는, -군]', '3.540'),\n",
    "     ('이', '[-다, -나, -었다, -지, -야]', '3.281'),\n",
    "     ('할', '[-만한, -지, -지도, -까봐, -수록]', '2.032'),\n",
    "     ('인', '[-지, -가, -데, -들, -데요]', '1.411'),\n",
    "     ('와', '[-의, -는, -도, -같이, -같은]', '1.398'),\n",
    "     ('과', '[-는, -도, -의]', '1.137'),\n",
    "     ('해', '[-서, -야, -진, -도, -서는]', '1.047'),\n",
    "     ('만', '[-한, -의, -큼, -을, -은]', '0.791'),\n",
    "     ('나', '[-마, -요, -면, -의, -구]', '0.723'),\n",
    "     ('제', '[-를, -는, -의, -가, -와]', '0.630'),\n",
    "     ('들', '[-은, -이, -의, -을, -에게]', '0.592'),\n",
    "     ('씩', '[-이나, -은, -을, -만, -이라도]', '0.589'),\n",
    "     ('도', '[-로, -가도, -다, -록]', '0.530'),\n",
    "     ('당', '[-하고, -했다, -할, -하게, -하거나]', '0.257'),\n",
    "     ('엔', '[-가, -지]', '0.240'),\n",
    "     ('률', '[-이, -을, -은, -도, -과]', '0.171'),\n",
    "     ('일', '[-까, -지라도, -지, -까요, -수록]', '0.154'),\n",
    "     ('냐', '[-며, -는, -면은, -면, -라고]', '0.132'),\n",
    "     ('랑', '[-은, -도]', '0.095'),\n",
    "     ('케', '[-하는]', '0.065'),\n",
    "     ('뿐', '[-인, -만, -이었다, -이다, -인데]', '0.031'),\n",
    "     ('별', '[-로, -로는]', '0.025'),\n",
    "     ('댁', '[-은, -이]', '0.018'),\n",
    "     ('함', '[-으로써, -을, -이, -에, -과]', '0.018'),\n",
    "     ('키', '[-로]', '0.012'),\n",
    "     ('께', '[-서, -서는, -서도, -선, -로]', '0.007'),\n",
    "     ('경', '[-에는, -에, -제, -제적]', '0.005'),\n",
    "     ('됨', '[-에, -으로써, -을, -이, -과]', '0.004'),\n",
    "     ('감', '[-이, -을]', '0.003'),\n",
    "     ('덜', '[-이]', '0.001'),\n",
    "     ('미', '[-국, -일]', '0.001'),\n",
    "     ('임', '[-을, -에도, -이, -은, -에]', '0.000'),\n",
    "     ('하', '[-고, -는, -게, -여, -지]', '0.000')]\n",
    "     \n",
    "그러므로 composed_josa의 첫글자들이 명사 후보의 맨 뒤에 붙어있으면 의심을 해보아야 한다\n",
    "\n",
    "    >>> jsub_set = {j[0] for j in composed_josa}\n",
    "\n",
    "    '가 감 경 과 께 나 냐 당 댁 덜 도 됨 들 랑 로 률 만 미 별 뿐 씩 에 엔 와 이 인 일 임 제 케 키 하 한 할 함 해'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(score=1.943, frequency=77, branching_entropy=0.406, feature_fraction=0.620, eojeol_fraction=0.000)\n",
      "Score(score=3.215, frequency=67, branching_entropy=1.944, feature_fraction=0.900, eojeol_fraction=0.388)\n"
     ]
    }
   ],
   "source": [
    "def is_NsubJ(s_, s, bes_=0.5, f_fraction=0.5, eojeol_fraction_s_=0.1):\n",
    "    return (s_.branching_entropy < bes_) and (s.frequency / s_.frequency > f_fraction) and (s_.eojeol_fraction < eojeol_fraction_s_)\n",
    "\n",
    "pretty(score_dict['떡볶'])\n",
    "pretty(score_dict['떡볶이'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [N + Jsub] + J: 대학생과 + 의\n",
    "\n",
    "    f(대학생) >> f(대학생과)\n",
    "    Right side branching entropy(대학생) ~= high\n",
    "    (과의 in Josa) and (대학생 in Noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(score=3.035, frequency=8678, branching_entropy=2.832, feature_fraction=0.600, eojeol_fraction=0.232)\n",
      "Score(score=1.959, frequency=852, branching_entropy=1.925, feature_fraction=0.553, eojeol_fraction=0.242)\n",
      "Score(score=6.244, frequency=26, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.962)\n"
     ]
    }
   ],
   "source": [
    "pretty(score_dict['대학'])\n",
    "pretty(score_dict['대학생'])\n",
    "pretty(score_dict['대학생과'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_jsubr_josa('상태로서', '의')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "과학활동 과학활동에\n",
      "Score(score=3.513, frequency=34, branching_entropy=1.655, feature_fraction=0.971, eojeol_fraction=0.000)\n",
      "Score(score=6.244, frequency=12, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.833)\n",
      "Nsub + J ?  False\n",
      "Jsub+R = J? True 1.000\n",
      "N + Jsub ?  True\n",
      "\n",
      "지역주민 지역주민과\n",
      "Score(score=2.527, frequency=75, branching_entropy=1.648, feature_fraction=0.576, eojeol_fraction=0.107)\n",
      "Score(score=6.244, frequency=10, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.900)\n",
      "Nsub + J ?  False\n",
      "Jsub+R = J? True 1.000\n",
      "N + Jsub ?  True\n",
      "\n",
      "예술 예술가\n",
      "Score(score=2.543, frequency=4386, branching_entropy=2.851, feature_fraction=0.672, eojeol_fraction=0.124)\n",
      "Score(score=2.781, frequency=603, branching_entropy=2.450, feature_fraction=0.792, eojeol_fraction=0.124)\n",
      "Nsub + J ?  False\n",
      "Jsub+R = J? False 0.436\n",
      "N + Jsub ?  False\n",
      "\n",
      "상태 상태로서\n",
      "Score(score=2.396, frequency=4624, branching_entropy=2.380, feature_fraction=0.991, eojeol_fraction=0.097)\n",
      "Score(score=6.244, frequency=20, branching_entropy=-0.000, feature_fraction=1.000, eojeol_fraction=0.700)\n",
      "Nsub + J ?  False\n",
      "Jsub+R = J? True 1.000\n",
      "N + Jsub ?  True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def is_NJsub(s_, s, bes_=0.5, bes=0.5, eojeol_fraction_s=0.5, f_fraction=0.3):\n",
    "    return (s_.branching_entropy > bes_) and (s.branching_entropy < bes or s.eojeol_fraction > eojeol_fraction_s)\n",
    "\n",
    "def fraction_of_jsubr_is_josa(l, jsub):\n",
    "    rdict = {r:f for r, f in lrgraph.get(l+jsub, {}).items() if r != ''}\n",
    "    f_sum = sum(rdict.values())\n",
    "    if not f_sum:\n",
    "        return 1.0 if jsub in josa_dict else 0.0\n",
    "    f_jsubj = sum([f for r,f in rdict.items() if (jsub+r) in josa_dict])\n",
    "    return (f_jsubj / f_sum)\n",
    "\n",
    "def is_jsubr_josa(l, jsub, min_fraction=0.6):\n",
    "    return fraction_of_jsubr_is_josa(l, jsub) > min_fraction\n",
    "\n",
    "for w_, w in [('과학활동', '과학활동에'), ('지역주민', '지역주민과'), ('예술', '예술가'), ('상태', '상태로서')]:\n",
    "    print(w_, w)\n",
    "    pretty(score_dict[w_])\n",
    "    pretty(score_dict[w])\n",
    "    print('Nsub + J ? ', is_NsubJ(score_dict[w_], score_dict[w]))\n",
    "    print('Jsub+R = J?', is_jsubr_josa(w_, w[len(w_):]), '%.3f'%fraction_of_jsubr_is_josa(w_, w[len(w_):]))\n",
    "    print('N + Jsub ? ', is_NJsub(score_dict[w_], score_dict[w]), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58102"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = dict(filter(lambda x:x[1].score > min_noun_score, score_dict.items()))\n",
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58102, 50285)"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = {}\n",
    "\n",
    "for w, s in sorted(candidates.items(), key=lambda x:len(x[0])):\n",
    "    n = len(w)\n",
    "    if n == 1:\n",
    "        nouns[w] = s\n",
    "        continue\n",
    "    # Case: 떡볶 + 이 --> remove 떡볶\n",
    "    if (w[-1] in postnoun) and (w[:-1] in nouns):\n",
    "        s_ = nouns[w[:-1]]\n",
    "        if is_NsubJ(s_, s):\n",
    "            del nouns[w[:-1]]\n",
    "    \n",
    "    # Case: [대학생 + 과] + 의 --> pass 대학생과 e = 1\n",
    "    # Case: [정국 + 으로] + 의 --> pass 정국으로 e = 2\n",
    "    NJsub = False\n",
    "    for e in range(1, max(5, n)):\n",
    "        l = w[:-e]\n",
    "        jsub = w[-e:]\n",
    "        if (l in nouns) and (jsub in josa_dict):\n",
    "            s_ = nouns[l]\n",
    "            if is_NJsub(s_, s):\n",
    "                NJsub = True\n",
    "                break\n",
    "            if 0 == sum((1 if (jsub+r) in josa_dict else 0 for r in lrgraph.get(l, {}).keys())):\n",
    "                NJsub = True\n",
    "                break\n",
    "    if NJsub:\n",
    "        continue\n",
    "\n",
    "    nouns[w] = s\n",
    "\n",
    "len(candidates), len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예술가 True\n",
      "과거 True\n",
      "정국 True\n",
      "취재 True\n",
      "개체 True\n",
      "배후 True\n"
     ]
    }
   ],
   "source": [
    "words = ['예술가', '과거', '정국', '취재', '개체', '배후']\n",
    "for w in words:\n",
    "    print(w, w in nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'함북  김성환  학제간  제야  영어강  지오그래픽  본모  미증유  앨리  각권  외향  스파르  자와  경영상  로버트  언어상  새한  권영길  각급  금융상  인조견  연의  전대미문  런닝머신  사회심리  피천득  푸나  열역학  한낱  벽산  타종  이향  환경주  내부자  이정연씨  무호흡  함경남도  주간한국  사업상  국제법상  천혜  백열  후천성  까자  초유  국립과  혈중  법률구조  검토위  문화교호간  불굴  민흘림  고미  한독  지역사  아방  제반  태환  외환수급  석박사  형사과  박광태  가격상  장상  란상  가고파  전자계  전업주  모스  성격상  재판과정에서  지역환경  비체계  김중권  쌍마  현종  혼외  통상교섭본부  영과  우르  창경  소수인  신강  만반  세계여성  이인국  문학비  레포  김근태  신체상  아이스하키  순위항목  아더  탐관  이혜경  가향  산업간  화랭이패  사회정  나노'"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'  '.join(tuple(zip(*sorted(nouns.items(), key=lambda x:x[1].score, reverse=True)[:100]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound: 소수 + [집단 + 의]\n",
    "\n",
    "    소수 + 집단의 (집단의 = 집단 + 의 인지 확인)\n",
    "    Noun score 대체\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction을 한 뒤, L-R graph 수정\n",
    "\n",
    "'소수집단/명사 + 의'를 알고 난 뒤, [소수 - 집단\\*]에 대한 링크를 모두 지워줘야 '소수'에 대한 판단이 명확해짐. repeatly extraction을 해야 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
